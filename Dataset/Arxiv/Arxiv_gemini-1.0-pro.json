[
    "This report describes a Theano-based implementation of AlexNet, a deep convolutional neural network architecture for large-scale visual recognition. The implementation is designed to take advantage of multiple GPUs to accelerate training and inference. The performance of the implementation is evaluated on the ImageNet dataset, achieving state-of-the-art results. The implementation is freely available as open source software.",
    "**Deep Narrow Boltzmann Machines: Universal Probability Approximators**\n\nDeep narrow Boltzmann machines (DBMs) are a type of probabilistic graphical model. We demonstrate that DBMs are universal approximators of probability distributions on the n-dimensional sphere. This means that DBMs can be used to represent any probability distribution on the sphere to arbitrary accuracy. Our proof is based on the fact that DBMs can be transformed into deep neural networks, which are known to be universal approximators. This result has implications for the use of DBMs in machine learning and Bayesian inference.",
    "**Abstract**\n\n**Learning Stochastic Recurrent Networks**\n\nThis work enhances recurrent neural networks with latent variables by leveraging advances in variational inference. By modeling the hidden state of the network as a stochastic process, we introduce an efficient method for handling uncertainty and improving generalization performance. Our approach combines the strengths of variational autoencoders with the sequential nature of recurrent networks, enabling the learning of complex and dynamic representations from sequential data. Extensive experiments demonstrate that our proposed stochastic recurrent networks outperform existing methods on various tasks, including language modeling, machine translation, and time-series prediction.",
    "This paper presents a general framework for online adaptation of optimization hyperparameters during training of deep neural networks. The framework, called \"hot swapping,\" allows for seamless switching between different sets of hyperparameters without interrupting the training process. This enables the optimization algorithm to adapt to changing data distributions, network architectures, and training conditions, leading to improved generalization performance. The framework is implemented in the open-source Python library, AdaptML, and is demonstrated on a variety of image classification tasks, showing significant performance improvements over conventional hyperparameter tuning approaches.",
    "Many modern multiclass and multilabel problems are characterized by increasingly large output spaces. This poses a challenge for traditional label embedding methods, which often assume a small and finite output space. In this paper, we propose a novel fast label embedding method, called FastText Label Embeddings (FTLE), that is specifically designed for large output spaces. FTLE leverages the success of fastText word embeddings and extends it to the task of label embedding. We show that FTLE can effectively capture the semantic similarity between labels and significantly improve the performance of multiclass and multilabel classification tasks over traditional label embedding methods.",
    "Accurate representational learning of both the explicit and implicit relationships within data is critical to the success of machine learning models. Deep learning models, in particular, have shown remarkable performance in a wide range of tasks, including image classification, natural language processing, and speech recognition. However, these models are often data-hungry and require large amounts of labeled data to achieve good performance. In many real-world applications, labeled data is scarce or expensive to obtain. This has led to the development of unsupervised learning methods, which can learn useful representations of data without the need for labeled data.\n\nDynamic Adaptive Network Intelligence (DANI) is a novel unsupervised learning method that is inspired by the human brain's ability to learn from unlabeled data. DANI uses a hierarchical architecture of self-organizing maps to learn a compressed representation of the data. The maps are organized in a way that preserves the topological relationships between the data points. This allows DANI to capture both the explicit and implicit relationships within the data, even when the data is high-dimensional and complex.\n\nDANI has been shown to be effective for a variety of tasks, including dimensionality reduction, clustering, and anomaly detection. In this paper, we present a detailed description of the DANI algorithm and provide experimental results that demonstrate its performance on a variety of datasets.",
    "Automatic speech recognition systems usually rely on spectral-based features, such as MFCC or PLP. These features have been shown to be effective for a wide range of speech recognition tasks. However, they are not specifically designed to capture the temporal structure of speech. In this paper, we propose a new method for learning linearly separable features for speech recognition using convolutional neural networks (CNNs). Our method is based on the idea that the temporal structure of speech can be captured by a series of convolutional filters. We show that our method outperforms state-of-the-art methods on a variety of speech recognition tasks.",
    "We describe the neural-network training framework used in the Kaldi speech recognition toolkit, which provides support for various training criteria such as maximum-likelihood (ML), minimum phone error (MPE), and discriminative training (DT). We discuss important practical aspects of DNN training, including data augmentation, optimization techniques, regularization methods, and decoding methods. We also introduce our recently developed parallel training framework for DNNs, which employs natural gradient updates and parameter averaging. This framework can significantly speed up DNN training on large datasets, and it achieves lower error rates compared to traditional parallel training methods.",
    "We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we leverage the geometry of the space of representations to construct a smooth manifold that captures the key invariances of the model. This manifold can then be used to visualize the model's behavior under different transformations, and to identify and correct any unintended biases or artifacts in the representation. We demonstrate the effectiveness of our method on a variety of image and language models, showing that it can significantly improve the models' robustness to noise and adversarial attacks, and their performance on downstream tasks.",
    "**Abstract:**\n\nIn this work, we provide a novel perspective on unsupervised deep learning through the lens of group theory. Our approach addresses fundamental questions:\n\n* **Why Does Deep Learning Work?** We posit that deep learning succeeds by uncovering group symmetries in data and exploiting them for efficient representation.\n* **What Representations Does It Capture?** Our framework reveals that deep learning cascades extract a hierarchy of group-like representations, progressing from low-order to higher-order invariant features.\n* **How Do Higher-Order Representations Emerge?** We demonstrate that higher-order representations arise through a process of group fusion and symmetry breaking, where multiple low-order symmetries combine to form more complex ones.\n\nOur group theoretic perspective provides insights into the underlying mechanisms of unsupervised deep learning and offers new avenues for understanding and exploiting the power of deep representations.",
    "We present a novel architecture, the \"stacked what-where auto-encoders\" (SWWAE), which integrates discriminative and generative learning for object recognition in cluttered scenes. The SWWAE consists of a stack of auto-encoders, each of which learns to encode an object's appearance and location while simultaneously reconstructing the object's image. The discriminative learning component of the SWWAE uses a convolutional neural network (CNN) to classify objects and to suppress background clutter. The generative learning component of the SWWAE uses a deep convolutional auto-encoder (CAE) to reconstruct objects.",
    "We investigate the problem of inducing word embeddings that are tailored for a particular bilexical prediction task. We propose a novel approach that leverages the bilexical information during the embedding induction process. Our approach is based on a deep neural network that learns to predict the bilexical relation between two words given their embeddings. We evaluate our approach on a variety of bilexical prediction tasks and show that it significantly outperforms existing methods.",
    "**Abstract**\n\nThis paper presents a generative model for deep convolutional learning. The model is based on a novel probabilistic pooling operator that captures the dependencies between features at different scales. The model is able to generate realistic images and can be used for unsupervised feature learning. Experiments on a variety of image datasets demonstrate that the model achieves state-of-the-art results in terms of image generation quality and feature learning performance.",
    "Motivated by recent advances in generative models, we present a model capable of generating images from captions. Our model employs an attention mechanism to align generated image regions with specific words or phrases in the input caption. This alignment allows the model to capture fine-grained details and generate images that are semantically consistent with the provided captions. Experiments on the COCO and Flickr30k datasets demonstrate that our model outperforms existing caption-based image generation methods in terms of image quality and caption-image alignment.",
    "Convolutional neural networks (CNNs) work well on large datasets, but labeled data is often hard to come by. For this reason, unsupervised and semi-supervised learning methods for CNNs are of great interest. In this paper, we propose a Bayesian convolutional neural network (BCNN) with Bernoulli approximate variational inference. The proposed BCNN can handle both supervised and unsupervised learning tasks, and it can also learn from data with missing labels. We demonstrate the effectiveness of the proposed BCNN on several image classification tasks.",
    "We propose a new method for creating computationally efficient convolutional neural networks (CNNs) by using low-rank filters. Low-rank filters have been shown to be effective in reducing the computational cost of CNNs without sacrificing accuracy. However, existing methods for creating low-rank filters are often complex and difficult to implement. Our method is simple and easy to implement, and it can be used to create low-rank filters for any type of CNN. We demonstrate the effectiveness of our method on a variety of image classification tasks, and we show that our method can achieve significant computational savings without sacrificing accuracy.",
    "Distributed representations of words have boosted the performance of many Natural Language Processing tasks. However, most existing models are still incapable of capturing word sense variations. This paper proposes a simple and efficient method to generate word sense representations. The method exploits the semantic structure of WordNet and introduces the concept of sense similarity to measure the similarity between two word senses. Word sense representations are then constructed as weighted sums of the context vectors of words that commonly occur in these two senses. Experimental results show that the proposed method outperforms previous sense representation models on several word sense disambiguation tasks.",
    "We propose Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs). DENN introduces diverse embedding layers, where each layer captures different semantic and syntactic information. By fusing these diverse embeddings, DENN enhances the LM's ability to represent complex and multifaceted language. Experimental results on various NLP tasks demonstrate DENN's superiority over existing LMs, achieving state-of-the-art performance in text classification, machine translation, and question answering.",
    "**Abstract**\nA standard approach to Collaborative Filtering (CF), i.e., the prediction of user ratings on items, relies heavily on observed interactions to derive user and item representations. However, in real-world scenarios, many users and items are new and have no observed interactions, leading to the cold-start problem. This research explores representation learning for cold-start recommendation, focusing on learning effective representations for new users and items from related data sources. We propose a knowledge graph-based approach that leverages existing knowledge about users and items to enhance their representations. Experimental results on real-world datasets demonstrate that the proposed approach significantly improves the accuracy of CF recommendations, particularly for new users and items.",
    "We present NICE, a deep generative model for high-dimensional data that captures complex dependencies. NICE extends the Independent Component Analysis (ICA) framework by using a deep neural network to model the non-linear relationships between the latent variables. Our model is flexible and can capture a wide range of distributions, including those with heavy tails and multimodal distributions. We demonstrate the effectiveness of NICE on a variety of datasets, including images, text, and financial data.",
    "Deep Linear Discriminant Analysis (DeepLDA) introduces a novel technique for learning linearly separable latent representations in high-dimensional data. Inspired by traditional LDA, DeepLDA employs a deep neural network architecture to extract hierarchical features and iteratively refine the latent space. By optimizing a discrimination-aware objective, DeepLDA achieves excellent performance in both supervised and unsupervised scenarios. It outperforms existing methods on various classification and clustering tasks, demonstrating its efficacy in extracting informative and discriminative latent representations.",
    "Layer-sequential unit-variance (LSUV) initialization is a simple and effective method for weight initialization in deep neural networks. LSUV initializes the weights of each layer in a sequential manner, ensuring that the activations of each layer have unit variance. This initialization method has been shown to improve the convergence and generalization performance of deep neural networks on a variety of tasks. In this paper, we provide a theoretical analysis of LSUV initialization and demonstrate its empirical benefits on a range of deep learning tasks.",
    "We propose a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The transformation can be used as a preprocessing step for density modeling, which can improve the performance of subsequent machine learning algorithms. We demonstrate the effectiveness of our transformation on a variety of image datasets, and show that it can lead to significant improvements in the accuracy of density models.",
    "This paper presents a novel architecture for convolutional neural networks that significantly accelerates feedforward execution. The proposed approach, called Flattened Convolutional Neural Networks (FCNNs), reduces redundancy in the network's architecture by flattening the convolutional layers into a single layer. This flattening operation enables efficient matrix multiplication operations, resulting in faster forward pass computations. Experimental results demonstrate that FCNNs achieve comparable or superior accuracy to conventional CNNs, while offering significant speed improvements of up to 20%. FCNNs are particularly well-suited for applications where real-time inference is critical, such as autonomous driving and mobile vision systems.",
    "Purine is a novel deep learning framework that employs bi-graph networks to enhance representation learning and task performance. Bi-graph networks incorporate both node-level and edge-level information, allowing Purine to capture complex relationships within data. The framework comprises a feature extractor that generates node and edge embeddings and a graph neural network module that propagates information across the bi-graph. Purine has demonstrated promising results on various real-world tasks, showcasing its potential for effective and versatile deep learning applications.",
    "**Abstract:**\n\nIn this paper, we introduce a novel model, the Variational Recurrent Auto-Encoder (VRAE), which leverages the strengths of Recurrent Neural Networks (RNNs) and Stochastic Gradient Variational Bayes (SGVB). VRAs capture the sequential dependencies in data using RNNs while simultaneously learning a compact and expressive representation by exploiting SGVB's variational inference framework. Our model provides a principled approach to unsupervised learning of sequential data and outperforms existing methods on various benchmark tasks, demonstrating its effectiveness in modeling complex and high-dimensional sequences.",
    "**Abstract:**\n\nCurrent work in lexical distributed representations maps each word to a point vector in low-dimensional space. These representations are learned from large text corpora and have been shown to capture semantic and syntactic relationships between words. However, many existing methods for learning word representations do not take into account the uncertainty associated with word meanings. In this paper, we propose a novel method for learning word representations that incorporates uncertainty estimation. Our method, called Gaussian Embedding, uses a Gaussian distribution to represent each word's meaning. We develop an algorithm for learning the parameters of the Gaussian distributions by minimizing a reconstruction loss function. We evaluate our method on a variety of tasks, including word similarity, word analogy, and text classification. Our results show that Gaussian Embedding outperforms existing methods on all of these tasks.",
    "**Abstract**\n\nMultipliers are the most space and power-hungry arithmetic operators in digital implementations of deep neural networks (DNNs). To reduce the computational cost of DNNs, researchers have explored low-precision multiplications, which offer significant savings in resources. This work investigates various low-precision multiplication schemes, including fixed-point, floating-point, and stochastic rounding, and evaluates their impact on DNN training accuracy and efficiency. Our results show that low-precision multiplications can achieve comparable accuracy to full-precision multiplications while significantly reducing resource consumption, making them a promising approach for deploying DNNs on resource-constrained devices.",
    "Multiple instance learning (MIL) can reduce the need for costly annotation in tasks such as medical image segmentation by allowing training to occur with weakly-labeled data, where labels are provided for the entire dataset but not for individual instances. In this paper, we propose a fully convolutional multi-class MIL model that combines the strengths of both fully convolutional networks and MIL to achieve highly accurate segmentation of medical images. Our model is based on an encoder-decoder architecture with skip connections, which enables the efficient extraction and propagation of multi-scale features. To incorporate MIL, we use a max-pooling operation to aggregate instance-level features into bag-level features, which are then used to predict the class label for the entire bag. We evaluate our model on two publicly available medical image datasets and demonstrate that it outperforms existing MIL and fully convolutional methods in terms of segmentation accuracy. Our model has the potential to significantly reduce the cost of data annotation for medical image segmentation tasks, making it more accessible to researchers and clinicians.",
    "**Abstract**\n\nNested dropout, a recently proposed method for ordering representation units in autoencoders, has demonstrated promising results. In this paper, we extend the nested dropout approach to compact convolutional neural networks (CNNs). Specifically, we propose a novel nested dropout CNN architecture that employs hierarchical dropout layers to capture the dependencies between different representation levels. The hierarchical dropout layers are designed to progressively reduce the redundancy in the feature maps while preserving their discriminative information. Experimental results on various image classification datasets demonstrate the effectiveness of our approach. Our proposed nested dropout CNN architecture achieves state-of-the-art performance on both CIFAR-10 and CIFAR-100 datasets, surpassing the existing compact CNN models in terms of both accuracy and compactness.",
    "Stochastic gradient algorithms have been the main focus of large-scale learning problems and they led to the discovery of many new algorithms. Recently, the advent of deep learning models has shown that these methods can also scale to extremely large modern datasets. However, the design of large-scale stochastic gradient methods is influenced by the fact that the gradients are computed online and hence are very noisy. In this paper, we hope to significantly alleviate this issue by proposing a novel regularization technique that can be easily applied to arbitrary stochastic gradient algorithms. We show that our approach can provide substantial improvements that the same order as state-of-the-art methods, especially when the training dataset is large",
    "**Abstract**\n\nWhen a three-dimensional object moves relative to an observer, a change occurs on the observer's retina. This change is due to the object's motion and the observer's viewpoint. The change in the retinal image can be described by a transformation matrix. This transformation matrix can be used to determine how the object's appearance changes as it moves.\n\nIn this paper, we study the transformation properties of learned visual representations. We show that the transformation properties of learned visual representations are consistent with the transformation properties of the retinal image. This suggests that learned visual representations are based on the same principles as human vision.\n\nOur findings have implications for the development of computer vision systems. By understanding the transformation properties of learned visual representations, we can develop computer vision systems that are more robust to changes in viewpoint and motion.",
    "**Clustering is Efficient for Approximate Maximum Inner Product Search**\n\n**Abstract:**\n\nEfficient Maximum Inner Product Search (MIPS) is an important task with wide applicability. However, existing MIPS methods suffer from high computational complexity, making them impractical for large-scale datasets. This paper proposes a novel clustering-based approach for approximate MIPS. Our method leverages clustering to partition the dataset into smaller subsets, which significantly reduces the search space.\n\nWe present a theoretical analysis to demonstrate the effectiveness of our method in reducing the search complexity, and we empirically evaluate its performance on real-world datasets. Our results show that the proposed method achieves comparable accuracy to existing MIPS methods, while offering significant speedup, especially for large datasets. This makes it a practical and efficient solution for approximate MIPS in various applications.",
    "The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a neural network and probabilistic model. VAEs can be powerful deep generative models, but they also suffer from the problem of posterior collapse. Importance weighted autoencoders (IWAE; Burda, Grosse, Salakhutdinov (2016)) tackle this problem by introducing a specific type of reparameterization trick. This involves introducing auxiliary latent variables that are marginalized over during training, which allows the model to better approximate the posterior distribution of the latent variables. IWAEs have been shown to achieve state-of-the-art results on a variety of generative tasks, including image generation, language modeling, and speech synthesis.",
    "This work investigates how using reduced precision data in Convolutional Neural Networks (CNNs) affects network accuracy and resource usage. We focus on reducing the precision of weights and activations, while maintaining the full precision of gradients. We propose a novel method for training reduced precision CNNs, which combines quantization, dynamic fixed-point scaling, and gradient scaling. We evaluate our approach on a variety of CNN architectures and datasets, and demonstrate that it can achieve similar accuracy to full precision networks while significantly reducing memory usage and computational cost.",
    "**Metric Learning Approach for Graph-Based Label Propagation**\n\nThe efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they are applied. However, constructing an optimal graph can be computationally expensive and time-consuming. In this paper, we propose a metric learning approach for graph-based label propagation. Our approach learns a metric that measures the similarity between instances, and then uses this metric to construct a graph. We show that our approach outperforms existing graph construction methods on a variety of semi-supervised learning tasks.",
    "**Abstract**\n\n**Order-Embeddings of Images and Language**\n\nHypernymy, textual entailment, and image captioning are often viewed as distinct tasks. Here, we propose a unified framework that views these tasks as special cases of a single problem: order-embedding of images and language. Our framework uses a deep neural network to learn an order-preserving embedding space for both images and language. This allows us to perform a wide range of tasks, including hypernymy detection, textual entailment, and image captioning, in a single end-to-end model. Our approach achieves state-of-the-art results on all three tasks, demonstrating the power of our unified framework.",
    "We propose local distributional smoothness (LDS), a new notion of smoothness for statistical models that penalizes the model's predictions for nearby inputs to be similar in distribution. This encourages the model to make consistent predictions for similar inputs, even when the true underlying distribution is complex and multimodal. To enforce LDS, we introduce virtual adversarial training (VAT), a method for generating adversarial examples that are not necessarily misclassified but that maximize the distributional discrepancy between the model's predictions for the original and adversarial inputs. Experiments on image classification and regression tasks show that LDS with VAT significantly improves the model's robustness to adversarial examples and noise, while also maintaining or improving accuracy on clean data.",
    "**Training Convolutional Networks with Noisy Labels**\n\nThe wide availability of extensive labeled datasets is pivotal for Convolutional Network (ConvNet) models to attain remarkable recognition capabilities in various domains. However, practical scenarios often present datasets with noisy labels, a common issue that jeopardizes the accuracy and performance of ConvNets. Noisy labels arise from various sources, including manual annotation errors, data corruption, or inherent uncertainty in target variables.\n\nThis research aims to address the challenge of training ConvNets with noisy labels, proposing a novel approach to improve recognition accuracy. The proposed method leverages a co-training strategy that iteratively purifies the noisy labels and enhances the generalization capability of ConvNets. Experimental results demonstrate the effectiveness of the proposed approach on benchmark datasets, resulting in improved performance compared to existing methods. These findings have significant implications for enhancing the robustness and applicability of ConvNets in real-world scenarios with noisy labels.",
    "We provide novel guaranteed approaches for training feedforward neural networks with sparse connectivity. We leverage a probabilistic interpretation of neural networks to devise a novel training scheme that provably minimizes a variational upper bound of the true risk. This leads to a regularized empirical risk minimization framework that encourages sparsity and generalizes well to unseen data. Numerical experiments demonstrate the efficacy of our method, which outperforms existing baselines in terms of both accuracy and sparsity.",
    "Discourse relations bind smaller linguistic elements into coherent texts. However, automatically identifying discourse relations is a challenging task due to linguistic complexity, ambiguity, and data sparsity. To address these issues, we propose a novel framework called Entity-Augmented Distributional Semantics for Discourse Relations (EA-DSDR). Specifically, we leverage external knowledge from a knowledge base to augment word embeddings with information about entities and their semantic relations. By incorporating this entity-level knowledge, our framework enhances the representation of discourse cues and improves the identification of discourse relations. We demonstrate the effectiveness of EA-DSDR on three discourse relation recognition datasets, achieving state-of-the-art results and outperforming competitive baselines. Our work highlights the importance of incorporating external knowledge into natural language processing tasks and provides a valuable resource for future research in discourse analysis.",
    "In this work, we propose a new method to integrate two recent lines of work: neural network-based semantic relation extraction and neural network-based representation learning. Our method jointly predicts and factorizes relations in text, and uses the factorized latent representations to induce a semantic representation for each text. We show that our method outperforms existing methods on several semantic representation learning tasks, including text classification, question answering, and relation extraction.",
    "The notion of metric plays a key role in machine learning problems such as classification, clustering, and regression. In this paper, we propose a new framework for learning algorithms that are robust to metric perturbations. We introduce the concept of $(\u03b5, \u03b3, \u03c4)$-good similarity functions, which measure the similarity between two data points based on their distance from a set of landmarks. We show that learning algorithms that use $(\u03b5, \u03b3, \u03c4)$-good similarity functions are robust to metric perturbations that satisfy certain conditions. We also provide empirical evidence to support our theoretical results.",
    "**Abstract**\n\nWe introduce the Multiplicative Recurrent Neural Network (MRNN) as a novel model for compositional meaning representation in language. The MRNN incorporates multiplicative interactions between elements, capturing hierarchical compositionality and enabling it to model the sequential and compositional nature of language. Unlike previous recurrent neural network architectures, which primarily rely on additive interactions, the MRNN's multiplicative operations allow for a more expressive and interpretable representation of compositional semantics. We demonstrate the MRNN's efficacy on language modeling tasks and show that it outperforms state-of-the-art models in capturing compositional structures and generating meaningful texts.",
    "**Abstract:**\n\nFinding minima of a real valued non-convex function over a high dimensional space is a challenging problem due to the presence of multiple local minima. This article explores various approaches for tackling this issue, including:\n\n* **Coordinate Descent Methods:** Iteratively optimize along each coordinate while holding others fixed.\n* **Trust Region Methods:** Restrict the search to a local trust region, ensuring convergence to a local minimum.\n* **Gradient-Free Methods:** Utilize derivative-free techniques such as particle swarm optimization and simulated annealing.\n* **Convex Relaxation and Reformulation:** Relax the non-convexity by introducing convex constraints or reformulating the problem as a convex optimization problem.\n\nThe article provides an overview of the strengths, weaknesses, and applications of these approaches. It also discusses recent advances and future research directions in high-dimensional optimization.",
    "We develop a new statistical model for photographic images, in which the local responses of a set of linear filters form a low-dimensional manifold. This manifold can be captured by a small number of basis functions, and the coefficients of these basis functions provide a compact representation of the image. We show that this model can account for a large fraction of the variance in natural images, and that it can be used to perform a variety of image processing tasks, such as denoising, compression, and segmentation.",
    "**Abstract**\n\nMost modern convolutional neural networks (CNNs) used for object recognition are built using the same architectural components: convolutional layers, pooling layers, and fully connected layers. While these components have been shown to be effective for a wide range of tasks, they can also be computationally expensive and require a significant amount of data to train.\n\nIn this paper, we propose a new CNN architecture that uses only convolutional layers, eliminating the need for pooling layers and fully connected layers. This new architecture, which we call the All Convolutional Net (ACN), is simpler and more efficient than traditional CNNs, while achieving comparable accuracy on a variety of object recognition tasks.\n\nWe demonstrate the effectiveness of the ACN on the CIFAR-10 and ImageNet datasets, and show that it achieves state-of-the-art results on both datasets. We also show that the ACN is more efficient than traditional CNNs, both in terms of training time and memory usage.",
    "Artificial neural networks typically have a fixed, non-linear activation function at each neuron. These functions introduce non-linearities into the network, allowing it to learn complex relationships in the data. However, the choice of activation function is often made heuristically, and there is no guarantee that the chosen function is optimal for the task at hand. In this paper, we propose a method for learning the activation function of each neuron in a deep neural network. Our method uses a differentiable approximation of the activation function, which allows us to train the network using backpropagation. We show that our method can improve the performance of deep neural networks on a variety of tasks, including image classification, object detection, and natural language processing.",
    "This paper introduces a greedy parser based on neural networks, which leverages a new compositional semantic representation where word meanings are encoded as vectors resulting from a word-by-word composition operation. The parser uses a recurrent neural network (RNN) to incrementally compute the composition of word meanings as it proceeds through the sentence, and then applies a linear classifier to predict the parse tree. The proposed parser achieves state-of-the-art performance on the standard benchmark datasets, and is significantly faster than existing neural parsers.",
    "Suitable lateral connections between encoder and decoder are shown to allow higher layers of a denoising autoencoder to learn increasingly invariant representations of the input. Such lateral connections are modulated by the output of the previous layer in the encoder. As a result, the lateral connections progressively emphasize the most \"interesting\" features extracted at each layer, leading to representations that are highly robust to noise and other image distortions.",
    "We introduce a new method for visualizing and refining the invariances of learned representations. Our method is based on visualizing the geodesics of the representation space, which are the shortest paths between two points. By analyzing the geodesics, we can identify the directions in which the representation is most invariant and make targeted modifications to the representation to improve its invariance. We demonstrate the effectiveness of our method on a variety of tasks, including image classification, object detection, and natural language processing.",
    "Genomics are rapidly transforming medical practice and basic biomedical research, providing insights into disease mechanisms, prognosis, and therapeutic response. However, translating genomic information into clinically meaningful predictions has remained challenging. Here, we leverage advances in machine learning to develop genomic representations that can predict clinical outcomes in cancer. Using a large dataset of cancer patients, we train deep learning models to learn high-dimensional representations of genomic profiles. These representations capture complex interactions between genomic alterations and clinical outcomes, enabling accurate prediction of survival, recurrence, and response to therapy. Our findings demonstrate the utility of genomic representations for personalized cancer medicine and provide a framework for developing interpretable and actionable models that can guide clinical decision-making.",
    "**Abstract**\nExisting approaches to combine both additive and multiplicative neural units either use a fixed assignment of the units to different layers or require a hard switch between the two modes of operation. We propose a differentiable transition that smoothly interpolates between additive and multiplicative neurons, enabling a continuous and adaptive blending of the two modes. Our approach leverages a novel activation function that combines a rectified linear unit (ReLU) with a multiplicative gain term, controlled by a differentiable parameter. This parameter regulates the relative contribution of additive and multiplicative components, allowing for seamless transitions between the two modes. Experimental results demonstrate the effectiveness of our differentiable transition in enhancing model performance on tasks that require both additive and multiplicative computations, such as image classification, object detection, and natural language understanding.",
    "**Scale Normalization: Addressing Improper Scaling in Deep Neural Networks**\n\nImproper scaling between layers in deep neural networks hampers training. This study proposes a novel normalization technique, Scale Normalization, to address this issue. By explicitly normalizing the scale of intermediate activations, Scale Normalization enhances stability and facilitates gradient propagation. Experiments demonstrate that the proposed method outperforms existing methods, leading to significant accuracy improvements on various image classification tasks. This work provides a valuable tool for training deep neural networks more effectively.",
    "**Abstract**\n\nStick-Breaking Variational Autoencoders (SB-VAEs) extend Stochastic Gradient Variational Bayes (SGVB) for posterior inference of the weights of a Stick-Breaking process. This allows for non-parametric representation of the data distribution, enabling SB-VAEs to capture complex hierarchical structures in data. Through a series of experiments on real-world datasets, we demonstrate that SB-VAEs outperform conventional VAEs in terms of predictive performance and representation quality.",
    "Unsupervised learning on imbalanced data is challenging because, when given imbalanced data, current models tend to pay more attention to majority class samples during learning, resulting in poor performance in distinguishing minority class samples. In this paper, we address this problem by proposing a novel unsupervised learning algorithm named the Structure Consolidation Latent Variable Model (SC-LVM). The SC-LVM leverages latent variables to encode the underlying data distribution, and introduces a structure consolidation regularization to capture the local structure of data, which encourages the latent representations of minority class samples to be more discriminative. Extensive experiments on multiple real-world datasets demonstrate that the proposed SC-LVM significantly outperforms state-of-the-art unsupervised learning algorithms on imbalanced data, especially in terms of minority class performance.",
    "Generative Adversarial Networks (GANs) have achieved remarkable success as deep generative models. GANs are based on a two-player game where a generator network and a discriminator network compete with each other. The generator learns to produce synthetic data that is indistinguishable from real data, while the discriminator learns to distinguish between real and synthetic data. This abstract presents an alternative perspective on GANs, viewing them as a method for estimating the density ratio between real and synthetic data. This perspective leads to new insights into the training of GANs and provides opportunities for improving their performance.",
    "This paper presents a novel approach to text classification using natural language processing (NLP) methods. Our approach, Learning to SMILE(S) (Short and Medium-length Instance Learner for Entities and Semantics), directly applies NLP techniques to the classification task, without the need for any intermediate feature extraction or representation. We evaluate our approach on a variety of text classification tasks, including sentiment analysis, spam filtering, and question answering, and demonstrate that it achieves state-of-the-art results on all tasks. Our approach is simple, efficient, and easy to implement, and it can be applied to any text classification task where the input is a sequence of words.",
    "We introduce a neural network architecture and a learning algorithm to produce factorized symbolic representations. Our approach is based on the continuation learning paradigm, which allows us to incrementally learn new concepts without forgetting previously learned ones. We show that our approach can learn a variety of visual concepts, including objects, scenes, and actions. We also show that our approach can be used to generate novel images and videos that are consistent with the learned concepts.",
    "This paper studies the eigenvalues of the Hessian matrix of a loss function in deep learning. We show that the eigenvalues of the Hessian can be used to characterize the stability of the training process and the generalization performance of the trained model. We also show that the Hessian can become singular during training, leading to convergence issues. We propose a novel regularization method that prevents the Hessian from becoming singular and improves the stability and generalization performance of deep neural networks.",
    "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The transformation is based on the generalized normal distribution, which is a flexible distribution that can capture a wide range of shapes, including Gaussian, heavy-tailed, and skewed distributions. We show that the transformation can effectively Gaussianize data from natural images, as measured by the kurtosis and skewness of the transformed data. We also show that the transformed data can be used to improve the performance of image denoising and image compression algorithms.",
    "Approximate variational inference has shown to be a powerful tool for modeling unknown complex probabilitydistributions. In this paper, we propose a novel variational inference method for online anomaly detection in high-dimensional time series data. Our method utilizes a Gaussian process prior to capture the temporal correlation and non-linearity in the data, and employs a variational autoencoder to approximate the posterior distribution. The proposed method is able to effectively learn the normal behavior of the time series and detect anomalies in real-time. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of our method in detecting anomalies with high accuracy.",
    "**Abstract**\n\nWe present a general problem setting for training and testing the ability of agents to seek information. The setting is based on a formal model of information-seeking agents, which we extend to include sequential decision-making and learning. We define a range of tasks that agents can be evaluated on, and we provide a set of problem generators that can be used to create new tasks. We also provide a set of baseline agents that can be used to compare against. Our goal is to provide a common platform for research on information-seeking agents, and to encourage the development of new algorithms and models for this challenging task.",
    "We propose an extension to neural network language models to adapt their prediction to the long-term history. We employ a continuous cache that stores the representations of a subset of past words, which are selected dynamically based on their relevance to the current prediction. We equip the cache with an attention mechanism that allows the model to retrieve the most relevant past representations. We also introduce a novel self-monitoring mechanism that improves the model's ability to decide which past words to keep in the cache. We conduct extensive experiments on three popular language modeling datasets and show that our approach outperforms competitive baselines and improves the model's ability to handle long-term dependencies.",
    "Recent progress in generative models has shown promising results in generating images from captions. However, most existing models rely on recurrent neural networks (RNNs) to encode the caption, which can be slow and inefficient. In this paper, we propose a novel model that uses an attention mechanism to generate images from captions. Our model is based on a convolutional neural network (CNN) encoder and a decoder that uses an attention mechanism to attend to relevant parts of the caption. We evaluate our model on the COCO dataset and show that it outperforms existing models in terms of image quality and diversity.",
    "We propose a framework for training multiple neural networks simultaneously. The parameters from all models are constrained such that their trace norm is bounded. This encourages the networks to share similar features, resulting in improved performance on multi-task learning problems. We apply our method to a variety of tasks, including image classification, object detection, and natural language processing. In all cases, we achieve state-of-the-art results, demonstrating the effectiveness of our approach.",
    "This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and achieves state-of-the-art performance on a variety of continuous control tasks. The agent uses a novel experience replay buffer that stores both transitions and the corresponding action-value estimates. This allows the agent to learn from past experiences even when the environment is non-stationary. The agent also uses a prioritized replay mechanism to focus on the most informative experiences. The resulting agent is able to achieve state-of-the-art performance on a variety of continuous control tasks, including the MuJoCo benchmark suite.",
    "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network (RNN) that is trained on a corpus of popular songs. The model learns to generate melodies, harmonies, and rhythms that are stylistically consistent with the input corpus. We evaluate our model on a variety of tasks, including generating new songs, interpolating between existing songs, and generating songs in different styles. Our results show that our model is able to generate high-quality pop music that is indistinguishable from human-generated music.",
    "**Early Methods for Detecting Adversarial Images**\n\nMany machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation modifies an input, such as an image, in a way that is imperceptible to humans but causes the classifier to make a mistake. Early methods for detecting adversarial images typically relied on handcrafted features, such as the presence of noise or other artifacts. However, these methods were often brittle and could be easily evaded by attackers. More recently, deep learning-based methods for detecting adversarial images have been developed. These methods are more robust and can achieve state-of-the-art performance.",
    "**Abstract:**\n\nWe propose a novel method for creating computationally efficient convolutional neural networks (CNNs) by employing low-rank filters. Our approach leverages the observation that many CNN filters can be decomposed into a low-rank matrix. By representing these filters in a low-rank format, we significantly reduce the computational cost of CNNs while maintaining their accuracy. Experiments on CIFAR-100 and ImageNet datasets demonstrate that our method achieves competitive performance compared to baseline CNNs while offering substantial computational savings. This work has implications for resource-constrained environments and applications where efficiency is paramount.",
    "**Abstract**\n\nIn deep neural network learning, weight initialization plays a crucial role in ensuring convergence and performance. This paper proposes a novel weight initialization method called Layer-Sequential Unit-Variance (LSUV) initialization. LSUV initializes the weights of each layer sequentially, ensuring that each layer has unit variance. This simple yet effective method addresses the issue of vanishing and exploding gradients, which commonly arise in deep neural networks. Experimental results demonstrate that LSUV initialization significantly improves the performance and stability of deep neural networks on a variety of tasks.",
    "This paper extends the neural dependency parsing model of Kiperwasser & Goldberg (2016) by adding a deep biaffine attention mechanism. The proposed model, called Deep Biaffine Attention for Neural Dependency Parsing, is able to capture long-range dependencies and produce more accurate dependency trees. The model is evaluated on the English Penn Treebank and the Universal Dependencies treebank, achieving state-of-the-art results on both datasets.",
    "Accurate representational learning of both the explicit and implicit relationships within data is critical to advancing artificial intelligence. Dynamic Adaptive Network Intelligence (DANI) is a novel architecture that leverages dynamic graph networks to capture the evolving and complex relationships within data. DANI integrates both explicit and implicit relationships, enabling a deeper understanding of data and the ability to make more accurate predictions. The architecture consists of a dynamic graph network that constantly adapts to changing data, an adaptive attention mechanism that focuses on the most relevant relationships, and a novel loss function that encourages the learning of both explicit and implicit relationships. Experimental results on various real-world datasets demonstrate that DANI outperforms state-of-the-art methods, achieving significant improvements in accuracy and efficiency.",
    "**DeepSphere: towards an equivariant graph-based spherical CNN**\n\nSpherical data is found in many applications, from computer graphics to medical imaging. By modeling the discretized sphere as a graph, we can develop equivariant graph-based spherical CNNs that respect the symmetries of the sphere. We present DeepSphere, a novel spherical CNN architecture that is based on a regular icosahedral graph. DeepSphere is equivariant to rotations and reflections of the sphere, and it can be used to solve a variety of tasks on spherical data, such as classification, segmentation, and registration. We demonstrate the effectiveness of DeepSphere on several benchmark datasets, and we show that it outperforms state-of-the-art methods on a variety of tasks.",
    "High computational complexity hinders the widespread usage of Convolutional Neural Networks (CNNs), especially in mobile and embedded devices. To address this challenge, numerous methods have been proposed to approximate CNNs. However, most of them are software-oriented and suffer from the overhead of memory access and data movement. In this paper, we propose a hardware-oriented approximation method for CNNs. Our method leverages the inherent sparsity and redundancy of CNNs to design an efficient hardware architecture. Specifically, we propose a novel pruning algorithm to remove redundant weights and a binarization scheme to reduce the bit-width of the remaining weights. We also develop a hardware-aware training algorithm to fine-tune the pruned and binarized CNNs. Extensive experiments demonstrate that our method achieves significant speedup with negligible accuracy loss.",
    "The diversity of painting styles represents a rich visual vocabulary for the construction of an effective representation of artistic style. To this end, we introduce a hierarchical generative model that captures the multi-scale statistical dependencies within an image while simultaneously enforcing a shared style across the entire image. Our representation consists of a hierarchy of style filters that operate on a feature map at progressively finer scales. Style embeddings, defined as the low-pass filtered style representation, provide a compact encoding of the overall style. We show how to manipulate our representation to create novel images that share a consistent style while exhibiting high visual quality. Using a dataset of images across a wide range of artistic styles, we demonstrate the advantages of our learned representation over existing approaches to style transfer.",
    "**A Minimalistic Approach to Sum-Product Network Learning for Real Applications**\n\nSum-Product Networks (SPNs) are a class of expressive yet tractable hierarchical graphical models. However, learning the structure of SPNs is challenging due to the large dimensionality of the search space. Existing methods often rely on heuristics, which can lead to suboptimal results.\n\nIn this article, we present a minimalistic approach to learning the structure of SPNs, called LearnSPN. Our approach is based on a greedy search algorithm that is guided by a novel objective function. We show that LearnSPN is able to learn the structure of SPNs effectively and efficiently.\n\nWe empirically evaluate our approach on a variety of real-world datasets. Our results show that LearnSPN outperforms existing methods in terms of both accuracy and efficiency. We also demonstrate the potential of SPNs learned with LearnSPN for real-world applications, such as object detection and anomaly detection.",
    "Recent research on deep neural networks has focused primarily on improving accuracy. However, this often comes at the cost of increased model size and computational complexity. SqueezeNet is a new deep neural network architecture that achieves AlexNet-level accuracy with 50x fewer parameters and a model size of less than 0.5MB. This makes SqueezeNet ideal for applications where model size and computational resources are limited, such as mobile devices and embedded systems.",
    "**Abstract**\n\nIn question answering, reasoning over multiple facts is essential for providing accurate answers. However, traditional neural question answering models struggle with this task as they often suffer from the combinatorial explosion problem. In this paper, we propose a novel approach, Query-Reduction Networks (QRNs), to tackle this challenge. QRNs iteratively refine the question representation by fusing evidence from the facts and reducing the search space for the answer. Through extensive experiments, we demonstrate that QRNs significantly outperform existing methods on a variety of question answering datasets, achieving state-of-the-art results.",
    "We present a language-agnostic approach for automatically generating multilingual data clusters. This technique employs natural language processing models and semantic similarity measures to create semantically coherent clusters of entities. These automated clusters enable the evaluation of distributed representations of text and the assessment of language model performance across multiple languages. Our approach allows for fair and reliable multilingual evaluations, contributing to the development of inclusive natural language processing systems.",
    "**Abstract**\n\nRecurrent neural networks (RNNs) are widely used for predicting temporal data due to their ability to capture long-term dependencies. However, the deep feedforward structure of RNNs can lead to vanishing or exploding gradients, making training difficult. We propose a novel training algorithm for RNNs that utilizes surprisal-driven feedback. This algorithm injects a feedback signal into the network based on the surprisal of the next prediction, which helps to maintain stability during training and improves generalization performance. We empirically evaluate our algorithm on a variety of tasks, including language modeling, machine translation, and speech recognition, and show that it outperforms existing state-of-the-art methods.",
    "Although Generative Adversarial Networks (GANs) achieve state-of-the-art results on a variety of generative tasks, they are prone to mode collapse, where they fail to capture the full diversity of the data distribution. Mode Regularized GAN (MoRe-GAN) addresses this issue by introducing a novel regularization term that encourages the generator to produce diverse samples. This is achieved by minimizing the distance between the real data distribution and the generated data distribution in the feature space of a pre-trained classifier. Experiments on image generation and natural language processing tasks show that MoRe-GAN outperforms existing GANs in terms of sample diversity and image quality.",
    "**EPOpt: Learning Robust Neural Network Policies Using Model Ensembles**\n\nSample complexity and safety are major challenges when learning policies with reinforcement learning for real-world, safety-critical applications. To address these challenges, we propose EPOpt, a novel algorithm that leverages model ensembles to learn robust neural network policies. By training an ensemble of diverse models, EPOpt improves the robustness of the policy to model uncertainty and environmental noise. Extensive experiments demonstrate that EPOpt significantly outperforms state-of-the-art reinforcement learning algorithms in terms of sample efficiency, robustness, and safety.",
    "**Abstract**\n\nDivnet introduces a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity using determinantal point processes, which capture the repulsion between similar neurons. This approach encourages the formation of a diverse network of neurons, each selective to different aspects of the input. Divnet achieves state-of-the-art results in network compression, significantly reducing the size of neural networks while maintaining performance. Additionally, Divnet enables fine-tuning of the network's diversity, allowing for tailored compression strategies for different tasks and datasets.",
    "The efficiency of graph-based semi-supervised algorithms depends on the graph of instances on which they operate. Therefore, it is crucial to construct a graph that captures well the similarities between instances. This paper presents a metric learning approach for graph construction that is based on the manifold assumption. We propose a novel loss function that encourages instances lying on the same manifold to have similar neighborhoods in the graph. The proposed method is evaluated on a variety of datasets and compared to several state-of-the-art graph construction methods. Results show that our approach consistently improves the performance of graph-based label propagation algorithms.",
    "**Abstract:**\n\nOverfitting in deep neural networks hinders performance by memorizing training data rather than learning general patterns. To address this, we present a novel decorrelation-based regularization technique. By minimizing the correlation between feature activations, our method encourages diversity and prevents over-reliance on specific features. This constrained optimization process promotes robust representations that generalize well to unseen data. The proposed technique is lightweight, computationally efficient, and applicable to various network architectures. Experimental results on image and text classification tasks demonstrate that our approach significantly reduces overfitting and improves generalization accuracy, outperforming existing regularization methods.",
    "Deep neural networks are commonly trained using stochastic non-convex optimization procedures, which are driven by minibatches of data. While these algorithms have been remarkably successful, they are also known to be slow and computationally expensive. This paper presents a novel online batch selection strategy that significantly speeds up the training process without sacrificing accuracy. The proposed strategy exploits the inherent redundancy in deep neural networks to identify and selectively exclude redundant samples from each minibatch. Experiments on several large-scale image classification and language modeling tasks demonstrate that our method can achieve significant speedups (up to 2x) while maintaining or even improving model accuracy.",
    "We present a scalable approach for semi-supervised learning on graph-structured data that is based on graph convolutional networks (GCNs). Our approach leverages the power of GCNs to learn from both labeled and unlabeled data, and it scales to large graphs with millions of nodes and edges. We demonstrate the effectiveness of our approach on a variety of semi-supervised learning tasks, including node classification, link prediction, and community detection.",
    "**Abstract:**\nWe propose the \"Energy-based Generative Adversarial Network\" (EBGAN) model, which reinterprets the discriminator as an energy-based model. The EBGAN simultaneously trains both the generator and the discriminator with an energy-based loss function, allowing it to model the underlying data distribution more accurately. Additionally, we introduce a new technique for estimating the energy of generated samples, which is crucial for training EBGANs. Experiments demonstrate that EBGANs achieve state-of-the-art results in generating high-quality samples and capturing the data distribution compared to traditional GANs.",
    "Recent research in the deep learning field has produced a plethora of new architectures. However, there is a lack of understanding of the design principles that underlie these architectures. This paper presents a set of deep convolutional neural network (CNN) design patterns that can be used to guide the design of new architectures. These patterns are based on the analysis of a large number of existing CNN architectures. The patterns are organized into three categories: structural patterns, architectural patterns, and optimization patterns. Structural patterns define the overall architecture of a CNN, while architectural patterns define the individual components of a CNN. Optimization patterns define the techniques that can be used to train a CNN. The design patterns presented in this paper can be used to design new CNN architectures that are more efficient, accurate, and robust.",
    "**Abstract**\n\nMachine comprehension (MC) tasks require understanding complex relationships between a query and a given context paragraph to answer the query. This paper introduces Bidirectional Attention Flow (BiDAF), a novel neural architecture for MC that effectively models these interactions. BiDAF utilizes two bidirectional attention mechanisms to capture the context-query and query-context alignments, which are then combined to generate a representation of the most relevant parts of the context for answering the query. This enables BiDAF to achieve state-of-the-art results on several MC datasets, demonstrating its robust performance in modeling complex interactions for MC.",
    "**Abstract**\n\nDespite advancements in model learning, posterior inference remains a significant challenge for Helmholtz Machines (HMs). To address this, we propose a novel Joint Stochastic Approximation (JSA) learning method for HMs. JSA simultaneously updates the recognition and generative weights of the HM using stochastic gradients, enabling efficient learning and accurate posterior inference. Our approach optimizes both the evidence lower bound (ELBO) and the likelihood, leading to improved model performance compared to existing methods. We demonstrate the effectiveness of JSA on various datasets, achieving state-of-the-art results while significantly reducing training time.",
    "Object detection with deep neural networks is often performed by passing a few thousand candidate object proposals through a network to process each individual proposal. Although efficient deep networks have been proposed recently, such an exhaustive computation is still time-consuming, especially when applied to high-resolution images or videos. To address this problem, we propose an on-the-fly network pruning algorithm that adaptively prunes unnecessary network operations during inference. Inspired by the observation that object detection networks tend to focus on small image regions around the object centers, we iteratively refine the object proposals and only activate the network operations responsible for processing these refined regions. By combining network pruning with a region proposal refinement strategy, our algorithm achieves significant inference time reduction with only a minor decrease in detection accuracy. On PASCAL VOC and MS COCO benchmarks, our method achieves a speedup of 2.5x to 5x compared to the baseline network, while maintaining comparable accuracy.",
    "**Exponential Machines: Modeling Interactions between Features**\n\nMany real-world problems involve complex relationships between features. Traditional machine learning models often struggle to capture these interactions effectively, leading to suboptimal performance. However, exponential machines, a novel class of models, have demonstrated remarkable abilities in handling such interactions. This abstract presents an overview of exponential machines and their applications in various domains, including natural language processing, computer vision, and recommendation systems. By leveraging exponential transformations and introducing explicit feature interactions, exponential machines can model complex relationships more effectively, leading to significant improvements in accuracy and efficiency.",
    "Deep Variational Bayes Filters (DVBF) are a novel approach for unsupervised learning of state-space models from raw data. DVBF leverages the expressive power of deep neural networks to capture complex relationships and dynamics in the underlying system. By combining variational inference with Bayesian filtering, DVBF enables uncertainty quantification and efficient learning. Experimental results demonstrate the effectiveness of DVBF in modeling chaotic systems, nonlinear time series, and complex dynamic processes, outperforming existing methods in various unsupervised learning tasks.",
    "Recent progress in deep learning for natural language processing has led to new approaches to building dialog systems. In this paper, we present a novel end-to-end goal-oriented dialog model that learns to map user utterances directly to system actions without relying on any hand-crafted rules or templates. Our model is trained on a large corpus of human-human dialogs and achieves state-of-the-art performance on several goal-oriented dialog datasets. Our model can generate natural language responses, handle complex dialog structures, and adapt to different domains without requiring any domain-specific knowledge.",
    "Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is a strategy for applying adversarial training to semi-supervised learning environments. In this work, we investigate the application of adversarial training methods to the task of semi-supervised text classification. We propose a novel adversarial training algorithm that leverages both labeled and unlabeled data to improve classification accuracy. Our algorithm utilizes a combination of adversarial examples and virtual adversarial training to regularize the model and encourage it to learn from the unlabeled data. We evaluate our algorithm on a variety of text classification datasets and demonstrate its effectiveness in improving classification accuracy compared to existing semi-supervised learning algorithms.",
    "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, density estimation, or learning the probability distribution of a dataset, is a fundamental task with applications in various domains. In this paper, we propose a novel approach to density estimation using Real Non-Volume Preserving (Real NVP) transformations. Real NVPs are invertible neural networks that allow for efficient and stable density estimation. Our approach combines the strengths of Real NVPs with a likelihood-based objective function, enabling the estimation of complex and multimodal distributions. We demonstrate the effectiveness of our method on various datasets, showing competitive performance compared to state-of-the-art approaches.",
    "**Abstract**\n\nThis paper investigates the view-invariant properties of Convolutional Neural Networks (CNNs). We analyze the structure of the view-manifold in the feature spaces implied by CNNs, providing insights into how CNNs achieve view invariance. By examining the activations of individual neurons and the relationships between them, we demonstrate the existence of neurons that are selectively responsive to specific views and orientations. We also show that the view-manifold is a smooth and continuous manifold, which explains the ability of CNNs to generalize to novel views. Our findings enhance our understanding of the internal representations learned by CNNs and provide guidance for designing more view-invariant models.",
    "Bilinear models provide richer representations compared to linear models and have been widely used in various applications. To further improve the expressiveness of bilinear models, this paper proposes a novel pooling method called Hadamard product for low-rank bilinear pooling (HP-LRBP). HP-LRBP leverages the Hadamard product, which element-wise multiplies two matrices, to capture the pairwise interactions between features. Moreover, we introduce a low-rank constraint on the resulting matrix to reduce computational cost and enhance generalization ability. Extensive experiments on several benchmark datasets demonstrate that HP-LRBP outperforms state-of-the-art bilinear pooling methods in various tasks, including image classification, object detection, and video action recognition.",
    "Reinterpreting Importance-Weighted Autoencoders: The standard interpretation of importance-weighted autoencoders is that they maximize a tighter lower bound on the variational lower bound for Bayesian inference. However, this interpretation is incomplete and does not fully capture the behavior of these models. In this paper, we propose a new interpretation of importance-weighted autoencoders as maximizing a regularized variational lower bound. This new interpretation provides a more complete understanding of the model and its behavior, and it suggests new ways to improve the performance of importance-weighted autoencoders.",
    "We present a generalization bound for feedforward neural networks in terms of the product of the network's spectral norm and the logarithm of its input dimension. Our bound is based on the PAC-Bayesian approach, and it holds for any loss function that is Lipschitz continuous in the network's weights. We show that our bound is tighter than existing bounds for neural networks with ReLU activations, and we demonstrate its effectiveness on a variety of benchmark datasets.",
    "In this paper, we propose to equip Generative Adversarial Networks with the ability to produce calibrated energy estimates for the samples they generate. Calibrated energy estimates provide a measure of confidence in the generated samples and can be used to improve the quality of the generated data. We demonstrate the effectiveness of our approach on a variety of tasks, including image generation, text generation, and audio generation.",
    "**Abstract:**\nThis work presents an efficient method for outlier detection using ensembles of neural networks obtained via variational inference. By leveraging the Bayesian framework, we introduce a novel approach that generates diverse networks within an ensemble, leading to improved detection accuracy. The proposed method provides a principled way to estimate uncertainties in the predictions and robustly identify outliers in high-dimensional data. Empirically, our method outperforms existing baselines on various real-world datasets, demonstrating its effectiveness in outlier detection tasks.",
    "**Abstract**\n\nLSTM networks often suffer from a large number of parameters, which can slow down training and limit their scalability. This paper presents two simple factorization tricks that can significantly reduce the number of parameters and accelerate the training of LSTM networks. These tricks are based on the observation that the weight matrices in an LSTM network can be decomposed into a product of smaller matrices. This decomposition reduces the number of independent parameters and allows for more efficient training. Experiments show that the proposed factorization tricks can significantly reduce the training time of LSTM networks without sacrificing accuracy.",
    "This paper presents observations and discussion of previously unreported phenomena discovered while training residual networks. We find that the loss function topology, as a function of learning rate, exhibits a rich and complex structure, with multiple local minima and saddle points. We show that this structure can be exploited to improve training by using cyclical learning rates, which allow the optimizer to explore different parts of the loss function topology.",
    "Machine learning models are often used at test-time subject to constraints and trade-offs not present during training, such as resource budgets and latency requirements. To address this, we propose training models with reinforcement learning to modify their behavior at test-time in response to performance feedback. We refer to this as reinforcement learning test-time adaptation. By framing test-time adaptation as a sequential decision-making problem, we can choose actions that balance multiple objectives. We consider two common objectives: accuracy and efficiency. We develop a novel algorithm to solve this problem and demonstrate its effectiveness on two real-world applications: image classification and neural machine translation.",
    "Adversarial examples, carefully crafted inputs designed to fool deep learning models, pose a significant threat to the robustness and reliability of these models. In this work, we delve into the landscape of adversarial attacks on deep policies, exploring the techniques and methodologies employed to generate and mitigate adversarial examples. We provide a comprehensive overview of the state-of-the-art in adversarial attack methods, highlighting their strengths and limitations, and discuss the challenges and opportunities for future research in this area. Our findings underscore the importance of developing robust and resilient deep policies that are resistant to adversarial manipulation, paving the way for their safe and trustworthy deployment in real-world applications.",
    "Variational Continual Learning (VCL) is a simple yet effective framework for addressing the continual learning problem. VCL leverages variational inference techniques to learn a distribution over model parameters that captures the evolving task distribution. This allows the model to adapt to new tasks while retaining knowledge from previous tasks. VCL is a general framework that can be used with a variety of deep learning models and learning algorithms. Experiments on image classification and natural language processing tasks demonstrate the effectiveness of VCL, outperforming state-of-the-art continual learning methods.",
    "**Automatically Determining the Optimal Size of a Neural Network for a Given Task Without Prior**\n\nNeural networks are powerful machine learning models that have been successfully used in a wide variety of applications. However, determining the optimal size of a neural network for a given task can be a challenging problem. Too small a network may not be able to learn the necessary patterns in the data, while too large a network may be prone to overfitting.\n\nIn this paper, we propose a new method for automatically determining the optimal size of a neural network for a given task. Our method is based on the principle of minimum description length (MDL). MDL is a statistical principle that states that the best model for a given dataset is the one that minimizes the total length of the description of the data and the model.\n\nWe apply MDL to the problem of determining the optimal size of a neural network. We show that the MDL-based method can automatically determine the optimal size of a neural network for a given task without prior knowledge of the data. We also show that the MDL-based method outperforms other methods for determining the optimal size of a neural network.",
    "**Abstract:**\n\nNatural Language Inference (NLI) tasks require an agent to determine the logical relationship between a premise and a hypothesis. Previous approaches to NLI have focused on textual similarity and entailment, but these methods often fail to capture the complex interactions between entities in the input text.\n\nThis paper proposes a novel approach to NLI called Interaction Space, which explicitly models the interactions between entities in the input text. Interaction Space is a vector space that represents the possible interactions between entities, and the NLI task is formulated as a classification problem in this space.\n\nExperiments on the benchmark SNLI dataset show that Interaction Space significantly outperforms existing methods on NLI tasks. This suggests that modeling the interactions between entities is a key factor for improving NLI performance.",
    "The ability to deploy neural networks in real-world, safety-critical systems is severely limited by the discovery of adversarial examples\u2014malicious inputs that can cause a model to misclassify even when they are nearly indistinguishable from legitimate data. We propose a novel method for generating provably minimally-distorted adversarial examples, i.e., examples that are guaranteed to be as close as possible to the original data while still causing the model to misclassify. Our method outperforms existing approaches in terms of both distortion and success rate on a variety of datasets and models, including image classification tasks on MNIST and CIFAR-10, and object detection tasks on Pascal VOC and MS COCO. Our work provides a valuable tool for researchers and practitioners working on the security and robustness of neural networks.",
    "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. As a result, we derive a Stick-Breaking Variational Autoencoder (SB-VAE) which can **generate data with discrete latent variables** via efficient amortised inference. We show that the SB-VAE outperforms existing methods on a range of synthetic, and large-scale real-world benchmarks.",
    "We propose a framework for training multiple neural networks simultaneously. The parameters from all models are jointly optimised using a trace norm regulariser, which promotes a low-rank solution. This approach encourages the models to share similar features, while still allowing them to specialise on different tasks. We demonstrate the effectiveness of our approach on a variety of multi-task learning tasks, achieving state-of-the-art results on many of them.",
    "This paper presents a stable actor-critic deep reinforcement learning agent with experience replay. The agent is based on the proximal policy optimization (PPO) algorithm, but with modifications to improve stability and sample efficiency. The agent uses a prioritized experience replay buffer to store past experiences, which are then used to update the actor and critic networks. The agent also uses a clipped objective function to prevent large updates to the actor network, which can lead to instability. The agent is evaluated on a variety of continuous control tasks, and is shown to achieve state-of-the-art performance on many of these tasks.",
    "**Abstract**\n\nMany machine learning classifiers can be fooled by adversarial perturbations, which are carefully designed modifications to an input that can cause the classifier to misclassify it. Early methods for detecting adversarial images focused on identifying these perturbations. We discuss three main approaches: **gradient-based methods**, which use the gradients of the classifier's output with respect to the input to detect perturbations, **frequency-based methods**, which analyze the frequency components of adversarial images, and **feature-based methods**, which use the classifier's internal features to identify perturbations. ",
    "**Not-So-Random Features: Fourier-Analytic Kernel Learning**\n\nWe propose a principled method for learning kernels based on their Fourier-analytic properties. Our method yields kernels that effectively capture important features in the data, leading to improved generalization performance. The key idea is to represent kernels as a sum of exponentials, which allows us to analyze their frequency and phase response. By optimizing these parameters, we can design kernels that are tailored to specific tasks, such as image classification or natural language processing. Experiments on real-world datasets demonstrate the superiority of our Fourier-analytic kernels over standard random feature methods and other kernel learning approaches.",
    "**Abstract:**\nDespite the dominance of recurrent neural networks (RNNs) in deep reading comprehension models, their sequential nature limits their efficiency. This paper introduces a novel approach using convolutional neural networks (ConvNets) for fast reading comprehension. ConvNets' parallel processing capabilities enable simultaneous feature extraction from all word sequences, resulting in improved reading speed and comprehension accuracy. Experiments on standard reading comprehension datasets demonstrate the superiority of ConvNets over RNNs, achieving significant time savings while maintaining high quality performance.",
    "This report evaluates the reproducibility of \"On the Regularization of Wasserstein GANs\". We confirm the main theoretical results and provide experimental results for the proposed regularization techniques. Our findings provide insights into the effectiveness and limitations of the proposed methods, contributing to the broader understanding of Wasserstein GANs.",
    "Variational Autoencoders (VAEs) were originally motivated as probabilistic generative models in which a latent representation captures abstract features of the data. However, standard VAEs typically assume independence between latent variables, which limits their ability to model complex relationships in the data. This work proposes a novel approach to enhance the modeling capabilities of VAEs by introducing a hierarchical structure in the latent space and allowing information to be traded between latents at different levels of the hierarchy. The proposed Hierarchical Variational Autoencoder (H-VAE) outperforms standard VAEs on various benchmark datasets, demonstrating its improved ability to capture complex relationships and generate more realistic data.",
    "Methods that learn representations of nodes in a graph play a critical role in network analysis and mining tasks. Inductive graph representation learning methods aim to generalize to unseen graphs, but most existing methods are either supervised or transductive, limiting their applicability in real-world scenarios. In this paper, we propose Deep Gaussian Embedding of Graphs (DGE-Graph), an unsupervised inductive graph representation learning method that learns node representations via ranking. DGE-Graph formulates the representation learning task as a ranking problem and leverages a deep Gaussian embedding model to learn node representations that preserve pairwise node similarities. The model is trained using a novel ranking-based objective function, which encourages the learned representations to maintain the relative order of node similarities in the original graph. Extensive experiments on various graph datasets demonstrate that DGE-Graph outperforms state-of-the-art inductive graph representation learning methods on node classification, link prediction, and graph classification tasks.",
    "This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is to train multiple models on different views of the same data, and then use the outputs of these models to form an ensemble. This ensemble is then used to make predictions on new data. We show that our technique can improve the accuracy of domain adaptation models on a variety of datasets.",
    "**Abstract:**\n\nMost machine learning classifiers are susceptible to adversarial examples, carefully crafted inputs designed to cause misclassification. This vulnerability poses significant challenges for robust AI systems. This paper presents a theoretical framework to analyze the robustness of (deep) classifiers against adversarial examples.\n\nThe proposed framework formalizes the notion of adversarial examples and defines key robustness metrics. It introduces the concept of \"certified robustness,\" where the classifier can provide a guarantee that its prediction remains correct under any adversarial perturbation within a specified range.\n\nThe framework establishes theoretical bounds on the robustness of classifiers, providing insights into the factors that influence their vulnerability to adversarial examples. It also suggests potential avenues for improving robustness through regularization techniques and architectural design.\n\nBy providing a theoretical foundation for understanding and improving the robustness of classifiers against adversarial examples, this framework contributes to the development of more reliable and secure AI systems.",
    "**Abstract**\n\nWe develop a general problem setting for training and testing the ability of agents to gather information from human informants. Our setting is designed to be scalable, versatile, and applicable to a wide range of real-world tasks. We evaluate a range of agents on our problem setting and show that they are capable of achieving good performance on a challenging information-seeking task.",
    "We propose an extension to neural network language models to adapt their prediction to the cache (history) of their recent predictions. Most existing language models are autoregressive and predict the next token based solely on all previously predicted tokens. This forces the model to produce a sequence of predictions that is consistent with the fixed prefix from which they started. We relax this constraint and introduce a continuous cache to keep track of the model's recent predictions. We then augment the model with a small network that takes as input the hidden state of the language model and the current cache, and outputs a bias term that is added to the model's logits. This bias term encourages the model to make predictions that are consistent with its recent predictions, while still allowing it to make independent predictions when necessary. We evaluate our method on a variety of language modeling tasks and show that it can significantly improve the model's performance.",
    "Generative adversarial networks (GANs) are successful deep generative models based on a two-player minimax game. They have achieved remarkable results in generating realistic images, music, and text. In this paper, we provide a novel interpretation of GANs from a density ratio estimation perspective, which connects GANs to a long line of work on density ratio estimation. We show that the optimal discriminator in GAN corresponds to the optimal density ratio estimator in the maximum likelihood sense, and we establish an empirical risk minimization interpretation of the GAN loss function. Our perspective unifies GANs with a wide range of existing density ratio estimation methods, provides insights into the training dynamics of GANs, and suggests new regularization strategies to improve the stability and convergence of GAN training.",
    "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network (RNN) that learns to generate songs from a large dataset of popular music. The model is able to generate songs in a variety of styles, including pop, rock, and electronic. We evaluate our model on a number of metrics, including melodic complexity, harmonic complexity, and rhythmic complexity. Our model outperforms existing state-of-the-art models on all of these metrics.",
    "**Abstract**\nThis article analyzes the eigenvalues of the Hessian before and after training in deep learning. We find that, contrary to common belief, the Hessian is not always positive semi-definite after training, but rather can have negative eigenvalues. This observation has implications for the performance of gradient-based optimization algorithms and suggests that the Hessian may be a useful diagnostic tool for detecting potential optimization issues.",
    "In this paper, we propose a new feature extraction technique for program execution logs. Our technique, called Semantic Embeddings for Program Behavior Patterns (SEPB), uses a deep neural network to learn a distributed representation of program behavior. This representation can be used to identify patterns in program execution, such as loops, branches, and function calls. SEPB is able to capture both the syntactic and semantic structure of program behavior, making it a powerful tool for program analysis and understanding.",
    "We compared the efficiency of the FlyHash model, an insect-inspired sparse neural network (Dasgupta et al., 2020), with a standard multilayer perceptron (MLP) on a vision-based route following task. The FlyHash model was found to be more efficient than the MLP in terms of both accuracy and computational cost. This suggests that insect-inspired sparse neural networks may be a promising approach for developing efficient and robust autonomous systems.",
    "**Abstract:**\n\nIn peer review, reviewers typically provide qualitative scores for paper submissions. To facilitate ranking and decision-making, these scores are often quantized. However, quantization can introduce bias and limit granularity. This paper proposes a novel method for integrating reviewer rankings into quantized scores. By utilizing the Kendall tau distance as a measure of ranking similarity, we create a continuous score that captures the reviewer's preference order without the limitations of quantization. Empirical results on real-world peer review data demonstrate the effectiveness of our approach in improving ranking accuracy and reliability compared to traditional quantized methods.",
    "**Abstract**\n\nMany recent studies have probed status bias in the peer-review process of academic journals and conferences. However, the impact of author metadata on peer review at leading machine learning conferences remains unknown. To address this gap, we perform a feature-rich, matched observational study of ICLR submissions from 2017\u20132022. Our results show that submissions from authors with more prestigious affiliations are more likely to be accepted. This relationship is attenuated but remains significant after matching on a range of author characteristics. The findings suggest that status bias is present in the ICLR review process and highlight the need for continued efforts to mitigate its effects.",
    "We present a variational approximation to the information bottleneck of Tishby et al. (1999). The information bottleneck is a trade-off between compression and information preservation. Our variational approximation is based on a deep neural network that learns to compress the input data while preserving the most important information. We show that our approach outperforms existing methods on a variety of tasks, including image compression, text compression, and speech compression.",
    "Attention networks have proven to be an effective approach for embedding categorical inference within a neural network architecture. However, existing attention networks do not explicitly model the structure of the input data, which can limit their performance on tasks where the input data has a complex or hierarchical structure. In this paper, we introduce Structured Attention Networks (SANs), a novel attention mechanism that explicitly models the structure of the input data. SANs use a tree-structured architecture to represent the input data, and they learn to attend to different parts of the input data based on their structural relationships. We evaluate SANs on a variety of tasks, including image classification, natural language processing, and machine translation. Our results show that SANs consistently outperform existing attention networks, and they achieve state-of-the-art results on several tasks.",
    "We are proposing to use an ensemble of diverse specialists, where speciality is defined according to the type of adversarial attack the specialist is trained to defend against. Each specialist is trained on a different subset of the training data, which is augmented with adversarial examples crafted using a specific attack type. By combining the predictions of these specialists, we aim to improve the robustness of the overall model against a wide range of adversarial attacks. Our approach is conceptually similar to defensive distillation, but instead of distilling knowledge from a single teacher model, we leverage an ensemble of diverse specialists, each with its own unique expertise in defending against a particular type of attack.",
    "Neural Phrase-based Machine Translation (NPMT) explicitly models the phrase-based translation process with neural networks. The model combines the strengths of phrase-based and neural machine translation, achieving state-of-the-art results on various language pairs.",
    "LR-GAN is an adversarial image generation model that takes scene structure and context into account. It is a layered recursive generative adversarial network (LR-GAN) that generates images by recursively generating layers of detail. The model is able to generate high-quality images with realistic textures and details.",
    "We describe a simple scheme that allows an agent to learn about its environment in a self-supervised manner, without the need for any external supervision or reward. The scheme is based on the idea of asymmetric self-play, in which the agent plays two distinct roles, a teacher and a student. The teacher tries to challenge the student, while the student tries to learn how to counter the teacher. By alternating between these two roles, the agent is able to learn both how to play the game and also how to teach it. We show that this scheme is effective for learning in a variety of environments, including Atari games and a simulated robotic environment.",
    "Maximum Entropy Flow Networks: Exploiting Statistical Structure in Network Flow Optimization\n\nMaximum entropy modeling is a flexible and popular framework for formulating statistical models given partial information. It has been successfully applied in various domains, including natural language processing, computer vision, and bioinformatics. In this work, we propose a novel framework for network flow optimization that leverages maximum entropy modeling to exploit statistical structure in the network. We formulate a maximum entropy flow problem that incorporates prior knowledge about the flow patterns and solves it using an efficient algorithm. Extensive experiments on real-world and synthetic networks demonstrate that our approach significantly outperforms existing network flow optimization methods, especially when the available information is sparse or noisy.",
    "**Abstract**\n\nCommAI aims to evaluate the first steps towards a useful general AI. With machine learning successfully applied to new daunting problems almost every day, general AI starts to become a more realistic goal. However, there is still much debate about what constitutes general AI and how to measure progress towards it. In this paper, we propose a new framework for evaluating general AI that is based on the concept of \"communicative competence.\" We argue that a general AI must be able to communicate effectively with humans in order to be truly useful. We present a set of challenges that general AI must overcome to achieve communicative competence, and we propose a set of metrics that can be used to evaluate progress towards these challenges. We believe that this new framework will help to advance the development of general AI and lead to the creation of more useful and capable machines.",
    "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language processing, computer vision, and social network analysis. However, traditional neural networks have static computation graphs, which limits their ability to adapt to new data or tasks. Dynamic computation graphs, on the other hand, allow the network to change its structure during training or inference, making them more flexible and powerful. In this paper, we introduce a new framework for deep learning with dynamic computation graphs. Our framework is based on a novel graph data structure called the Dynamic Graph, which can efficiently represent graphs of arbitrary size and shape. We also introduce a new set of operators that can be used to create and manipulate Dynamic Graphs. Our framework is implemented in TensorFlow and is available as an open-source library. We demonstrate the effectiveness of our framework on a variety of tasks, including natural language processing, computer vision, and social network analysis.",
    "Although deep learning models have proven effective at solving problems in natural language processing, they often suffer from a lack of interpretability. This makes it difficult to understand how these models make predictions and can lead to errors going unnoticed. In this paper, we propose a method for extracting rules from Long Short Term Memory (LSTM) networks. Our method uses a combination of attention mechanisms and gradient-based optimization to identify the most important input features and the relationships between them. We evaluate our method on a variety of natural language processing tasks and show that it can significantly improve the interpretability of LSTM networks without sacrificing accuracy.",
    "**Abstract:**\n\nDeep reinforcement learning has achieved impressive results, but tasks with sparse rewards pose significant challenges. Stochastic neural networks (SNNs) offer a promising approach by introducing noise and uncertainty into the learning process.\n\nThis paper explores the use of SNNs for hierarchical reinforcement learning (HRL). HRL decomposes complex tasks into a hierarchy of subtasks, allowing for more efficient and flexible learning. We develop a novel SNN-based HRL framework that incorporates both compositional and meta-learning components.\n\nExperiments on challenging HRL environments demonstrate that our framework outperforms existing methods in terms of both sample efficiency and task generalization. SNNs provide a powerful tool for handling the uncertainty and sparsity inherent in HRL, enabling agents to learn complex behaviors with fewer interactions with the environment.",
    "**Abstract:** Deep generative models have achieved impressive success in recent years. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) are two representative architectures that have shown promising results in generating realistic images, videos, and other types of data. However, these models often suffer from mode collapse or produce blurry images. To address these limitations, we propose a novel unifying framework that combines the strengths of GANs and VAEs. Our framework, named Unified Generative Adversarial Network (UGAN), leverages the adversarial training of GANs to enforce global consistency while utilizing the latent variable modeling of VAEs to capture local details and prevent mode collapse. Experiments on various datasets demonstrate that UGAN outperforms existing generative models in terms of image quality, diversity, and perceptual realism.",
    "We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple yet effective method that enhances the reliability of out-of-distribution image detection. ODIN is based on the idea of temperature scaling, which has been shown to improve the calibration of neural networks. We show that ODIN can significantly improve the performance of out-of-distribution image detection on a variety of datasets and models.",
    "**Abstract:**\n\nThis framework introduces an information-theoretic approach for unsupervised learning of representations in large-scale neural networks. It leverages the Infomax principle to maximize the mutual information between the input data and the neural population activity, leading to representations that capture the underlying structure and dynamics of the data. The framework enables fast and robust learning through a novel algorithm and provides theoretical guarantees for its performance. Experiments on large-scale datasets demonstrate the effectiveness of the framework in extracting informative representations, outperforming existing unsupervised learning methods. This approach has potential applications in various domains requiring unsupervised feature learning, such as computer vision and natural language processing.",
    "Recurrent Neural Networks (RNNs) have demonstrated exceptional performance in sequence modeling tasks. Despite their effectiveness, training RNNs can be computationally intensive due to the sequential nature of their state updates. To address this challenge, Skip RNNs were introduced, a novel architecture that learns to selectively skip state updates during training. By incorporating a skip connection between the current and future hidden states, Skip RNNs can efficiently handle long-range dependencies while reducing the computational burden associated with unnecessary state updates. This approach enables faster training and convergence, making Skip RNNs a promising solution for resource-constrained environments or applications where real-time performance is crucial.",
    "**Partial Warm Restarts for Gradient Descent**\n\nRestart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts, where only a subset of the model parameters are reinitialized, can be beneficial in gradient-based optimization to escape local minima and improve generalization performance. This paper introduces SGDR (Stochastic Gradient Descent with Warm Restarts), a simple and effective algorithm that incorporates partial warm restarts into stochastic gradient descent. SGDR operates by periodically resetting a fraction of the model's weights to their initial values, while preserving the remaining weights and gradients. Experiments demonstrate that SGDR outperforms standard stochastic gradient descent on a variety of tasks, including image classification, object detection, and natural language processing.",
    "Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. However, their performance can be sensitive to the choice of action-dependent control variates, which are used to reduce variance in the policy gradient estimator. In this work, we propose a novel approach for learning action-dependent control variates that is based on Stein's identity. Our approach utilizes a neural network to learn a control variate that is specifically tailored to the given policy and task. We demonstrate the effectiveness of our method on a range of benchmark reinforcement learning tasks, showing that it can significantly reduce variance and improve the performance of policy gradient methods.",
    "Skip connections have revolutionized the training of deep neural networks, making it possible to train networks with hundreds or even thousands of layers. These connections allow gradients to flow directly from the input to the output of the network, preventing the vanishing gradient problem that plagued earlier deep networks. As a result, skip connections have enabled the development of deep learning models that achieve state-of-the-art performance on a wide range of tasks, including image recognition, natural language processing, and speech recognition.",
    "We have tried to reproduce the results of the paper \"Natural Language Inference over Interaction Space\" presented at ICLR 2018. The paper introduces a novel neural network architecture for natural language inference tasks based on matrix-vector multiplication.  We attempted to reproduce the results reported in the paper, but were unable to achieve the same level of performance. We believe that this may be due to differences in our experimental setup or implementation, and we encourage others to try to reproduce these results to confirm our findings.",
    "**Reproduction Report on \"Learn to Pay Attention\"**\n\nWe have successfully implemented the \"Learn to Pay Attention\" model of attention mechanism in convolutional neural networks. Our implementation replicated the results reported in the original paper, demonstrating that the model can effectively improve the performance of convolutional networks on image classification tasks. We believe that our implementation can serve as a useful resource for researchers and practitioners interested in exploring the potential of attention mechanisms in computer vision.",
    "**Abstract**\n\nWe introduce SufiSent, a novel framework for computing universal sentence representations using suffix encodings. SufiSent leverages the power of suffix encoding, a technique that captures the semantic and syntactic structure of words, to generate dense and meaningful vector representations of sentences. These representations are language-agnostic, enabling their use in a wide range of cross-lingual applications such as machine translation, text classification, and information retrieval. SufiSent's simplicity and efficiency make it a viable and effective solution for tasks requiring universal sentence representations.",
    "In neural networks, polynomial features often enhance representation. However, they can suffer from unstable gradients if the input features vary in scale. This paper proposes a simple yet effective scheme for scaling polynomial features, ensuring stable gradients and improved representation matching. Experiments demonstrate that our method significantly boosts the performance of polynomial feature-based models on various tasks.",
    "**Abstract**: We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the network's weight matrices and a PAC-Bayesian term. This bound is tighter than existing bounds for neural networks, and it can be used to derive new results on the generalization of neural networks.",
    "Uncertainty Estimation via Stochastic Batch Normalization\n\nIn this work, we investigate the Batch Normalization (BN) technique and propose its probabilistic interpretation. We propose a novel method for uncertainty estimation by introducing stochasticity into BN. Our method, called Stochastic Batch Normalization (SBN), incorporates a reparametrization trick to introduce a Gaussian distribution to the BN parameters. This allows us to estimate the uncertainty of the model's predictions by sampling from the posterior distribution of the BN parameters. Experimental results on image classification tasks demonstrate that SBN outperforms existing uncertainty estimation methods and achieves state-of-the-art results on multiple datasets.",
    "It is widely believed that the success of deep convolutional networks is based on progressively increasing the network depth. To explore alternative approaches, we propose the i-RevNet, a deep invertible network, which removes the bottleneck of information loss in deep convolutional layers. We propose to employ the invertible building block as a basic unit for the construction of deep neural networks. With the invertible building block, the i-RevNet is theoretically able to capture both high-level and low-level features simultaneously. In addition, by adding skip connections between the invertible blocks, the i-RevNet can effectively train very deep networks. We validate the effectiveness of the i-RevNet on image classification and image generation tasks. Experimental results demonstrate that the i-RevNet outperforms the ResNet and DenseNet with the same depth.",
    "Deep latent variable models are powerful tools for representation learning. In this paper, we adopt the Information Bottleneck (IB) framework to learn sparse latent representations that minimize the mutual information between the latent variable and nuisance factors. We introduce a new deep IB loss function based on the copula, a multivariate cumulative distribution function, which estimates the mutual information without requiring density estimation. Our method, called Deep Copula IB (DCIB), outperforms existing deep IB approaches on various datasets, demonstrating the effectiveness of copula-based mutual information estimation for learning sparse latent representations.",
    "**Abstract**\n\nWe introduce a variant of the MAC model (Hudson and Manning, ICLR 2018) with attention and question-answering features, designed for transfer learning tasks. Experiments on three datasets show that our model outperforms the original MAC model on two of the datasets, and achieves comparable performance on the remaining one. This suggests that our model is a promising option for transfer learning applications.",
    "Adaptive Computation Time (ACT) for Recurrent Neural Networks (RNNs) has emerged as a promising architecture that dynamically adjusts the computation time for each hidden state during training. Unlike conventional RNNs with fixed computation time, ACT enables more efficient training by selectively allocating computational resources to complex sequences. This abstract provides a brief overview of the ACT architecture, its key advantages, and potential applications in various domains.",
    "**Efficient GAN-Based Anomaly Detection**\n\nGenerative adversarial networks (GANs) are able to model the complex high-dimensional distributions of real-world data. This makes them well-suited for anomaly detection, as anomalies are instances that deviate from the expected distribution. However, traditional GAN-based anomaly detection methods are often computationally expensive.\n\nIn this paper, we propose an efficient GAN-based anomaly detection method by utilizing a fast approximation to the Wasserstein distance. Our method leverages a combination of a deep neural network and a variational autoencoder to generate realistic samples from the normal distribution. By comparing the generated samples to the real data, we can detect anomalies with high accuracy and low computational cost.\n\nThe results on several benchmark datasets demonstrate that our proposed method outperforms state-of-the-art GAN-based anomaly detection methods in terms of both accuracy and efficiency.",
    "**Abstract**\n\nNatural Language Inference (NLI) over Interaction Space proposes a novel approach to NLI by reformulating it in a probabilistic framework that leverages interactions between input sentences. By modeling the relationship between sentences as a probability distribution over a space of possible interactions, the model can capture the complex and multifaceted nature of the NLI task. This approach outperforms previous methods and provides a deeper understanding of the inference process involved in NLI.",
    "**Provably Minimally-Distorted Adversarial Examples**\n\nThe ability to deploy neural networks in real-world, safety-critical systems is severely limited by the existence of adversarial examples, which are carefully crafted inputs designed to fool a network into making a wrong prediction. To address this challenge, we propose a novel technique for constructing provable minimally-distorted adversarial examples. Our method leverages recent advances in differential privacy to ensure that the generated adversarial examples are indistinguishable from legitimate inputs to the human eye. This enables the development of safety-critical systems that are robust to adversarial attacks while maintaining human interpretability.",
    "**Abstract:**\n\nDeep Neural Networks (DNNs) have revolutionized predictive modeling due to their ability to capture complex relationships within data. However, the black-box nature of DNNs often limits their explainability and interpretability. This study proposes hierarchical interpretations for neural network predictions, enabling users to understand the decision-making process at multiple levels of granularity. The proposed approach decomposes DNN predictions into a hierarchy of interpretable components, providing insights into the contributions of individual features, interactions among features, and the overall model structure. This hierarchical interpretation enhances the comprehensibility and trustworthiness of DNN predictions, facilitating their use in critical decision-making tasks.",
    "In this work, we address the musical timbre transfer problem, aiming to transform audio clips to sound as if they were played by a different instrument. We introduce a novel pipeline, TimbreTron, featuring WaveNet, CycleGAN, and constant-Q transform (CQT), to perform the transformation. TimbreTron combines cutting-edge audio generation and style transfer techniques to empower musicians, producers, and artists with unprecedented sound manipulation capabilities.",
    "We consider the task of word-level language modeling and study the possibility of combining hidden-states-based and context-based features to improve the performance of dynamical language models. We propose a novel meta-learning approach that learns to weight these features adaptively based on the input data. Our approach outperforms existing methods on a variety of benchmark datasets, demonstrating the effectiveness of our meta-learning approach.",
    "GANs (Generative Adversarial Networks) are powerful generative models that can model the manifold of natural images. However, GANs are often unstable and can be difficult to train. In this paper, we explore the use of manifold regularization to stabilize GANs and improve their performance. We show that manifold regularization can help GANs to learn more discriminative and realistic images, and that it can also improve the stability of GAN training.",
    "**Abstract**\n\nWe identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss that exhibit no bad local valleys in their loss landscape. This implies that for these networks, any local minimum is a global minimum, and gradient descent will always converge to the optimal solution. This result provides theoretical support for the empirical observation that deep neural networks often generalize well even when trained with gradient descent.",
    "Visual Question Answering (VQA) models have struggled with counting objects in natural images so far. This is because counting requires reasoning about the spatial layout of objects and their relationships to each other. In this paper, we present a new VQA model that can learn to count objects in natural images. Our model uses a graph convolutional network (GCN) to represent the spatial layout of objects in the image. The GCN learns to aggregate information from neighboring objects, which allows it to reason about their relationships to each other. We show that our model significantly outperforms previous VQA models on the task of counting objects in natural images.",
    "**Abstract:**\n\nOne of the challenges in studying generative adversarial networks (GANs) is their instability during training. This instability often manifests as mode collapse, where the generator collapses to producing a single mode or category of data.\n\nSpectral normalization (SN) is a technique proposed to address this issue. SN enforces a Lipschitz constraint on the weights of the network, which helps to stabilize the training process and prevent mode collapse. In this paper, we provide a concise overview of SN and discuss its benefits and limitations for GAN training.",
    "**Abstract**\n\nEmbedding graph nodes into a vector space enables leveraging machine learning techniques for node classification. Numerous node embedding algorithms exist, each with its unique characteristics and performance. This study investigates the relationship between node centralities and classification performance for various node embedding algorithms. Centralities measure the importance of nodes within a graph, providing insights into their structural properties. The study evaluates how node centralities influence the classification accuracy of embedded nodes. The results show that node centralities can provide valuable information for understanding and characterizing node embedding algorithms, offering a basis for choosing appropriate algorithms for specific classification tasks.",
    "We introduce a new dataset of logical entailments for the purpose of measuring models' ability to make inferences that are logically valid. This dataset contains 2000 entailment pairs, each consisting of a premise and a hypothesis. The premise is a set of statements that are logically consistent, and the hypothesis is a statement that is either true or false given the premise. We evaluate the performance of several different neural network models on this dataset, and we find that the best model achieves an accuracy of 93.5%. This suggests that neural networks are capable of making logical inferences, and that they can be used to build systems that can reason about the world.",
    "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, leading to significant savings in storage and computational costs. However, these techniques often rely on heuristics or iterative procedures, making it challenging to find the optimal pruning strategy. In this work, we propose a novel lottery ticket hypothesis that posits that a large, randomly-initialized network contains a much smaller subnetwork that can be trained to achieve similar accuracy as the original network. This subnetwork, which we refer to as the winning ticket, can be identified by pruning the original network and retraining the resulting subnetwork. We provide both theoretical and empirical evidence to support our hypothesis and demonstrate that the winning ticket can be found efficiently using a simple iterative pruning algorithm. Our results suggest that the lottery ticket hypothesis can provide a principled approach to finding sparse, trainable neural networks, leading to significant improvements in efficiency and interpretability.",
    "This paper characterizes the singular values of the linear transformation associated with a standard 2D multi-channel convolutional layer. We show that the singular values are concentrated around a single value, which we call the singular value of the layer. We also show that the singular value is determined by the filter size, the number of input and output channels, and the padding and stride of the convolution. Our results provide a theoretical foundation for understanding the behavior of convolutional layers and can be used to improve the design and training of deep neural networks.",
    "**Abstract:**\n\nThis paper presents a theoretical framework for deep locally connected networks with ReLU activations. We first establish the universal approximation capability of these networks and derive upper bounds on their approximation error. We then analyze the generalization error of these networks under the assumption that the data is generated by a Gaussian mixture model. Our theoretical results provide insights into the behavior of deep locally connected networks and suggest that they may be well-suited for tasks involving complex and structured data.",
    "Neural Program Search (NPS) is an algorithm that generates programs from natural language descriptions and examples. NPS takes a natural language description of a programming task and a set of input-output examples, and generates a program that solves the task. NPS is based on a neural network that learns to map natural language descriptions and examples to programs. NPS can be used to solve a wide range of programming tasks, including sorting, searching, and string manipulation. NPS is a powerful tool that can help to automate the process of programming.",
    "Most state-of-the-art neural machine translation systems, despite being different in architectural skeletons (e.g. recurrence, convolutional), use an attention-based encoder-decoder architecture for learning phrase-level correspondences between source and target languages. However, existing attentions mainly focus on word-level matches and thus fail to capture meaningful phrases consisting of multiple consecutive words, resulting in inadequate translation quality. To address this issue, we propose Phrase-Based Attentions (Phrase-Attn) that explicitly attend to phrases in the target language and exploit word-phrase complementary information to produce more accurate phrase-level alignments. Specifically, Phrase-Attn constructs a phrase-aware representation for each source token which aggregates the information from aligned target phrases under different attention spans. Experiments on three standard translation datasets demonstrate the effectiveness of our proposed Phrase-Attn, which significantly improves the translation quality by capturing long-range dependencies and rich semantic information.",
    "**Abstract**\n\nWe introduce the problem of learning distributed representations of edits. By combining a \"neural editor\", which produces edits given two representations, with a \"neural learner\", which learns representations of edits, we show that it is possible to learn to represent edits in a way that is both meaningful and useful for downstream tasks. We demonstrate the effectiveness of our approach on a variety of tasks, including code editing, text editing, and image editing.",
    "We propose a principled method for kernel learning, which relies on a Fourier-analytic characterization of positive definite functions. By enforcing a positivity constraint on a kernel, our method ensures that the learned kernel is valid for use in machine learning tasks such as kernel methods and Gaussian processes. The proposed method is based on the idea of doubly stochastic gradients, which allows us to efficiently optimize the kernel parameters while maintaining positivity. Experiments on several synthetic and real-world datasets show that our method outperforms existing kernel learning methods in terms of predictive performance and computational efficiency.",
    "Variational Continual Learning (VCL) is a novel framework for continual learning that leverages variational inference to learn a distribution over model parameters. This distribution is updated incrementally as new tasks are encountered, allowing the model to retain knowledge from previous tasks while adapting to new ones. VCL is simple to implement and can be applied to a wide range of continual learning problems. Experiments on several benchmark datasets demonstrate that VCL outperforms existing continual learning methods, achieving state-of-the-art results in both accuracy and forgetting.",
    "This report investigates the reproducibility of \"On the Regularization of Wasserstein GANs\" by Miyato et al. (2018). We replicated the experiments described in the original paper, using the provided code and datasets. We found that the reported results could not be fully reproduced, highlighting the importance of careful experimental validation in machine learning research.",
    "**Abstract:**\n\nWe introduce a novel feature extraction method for program execution logs using semantic embeddings. This method captures program behavior patterns by generating dense vector representations that encode the semantics of the executed instructions. By leveraging these embeddings, we enable the detection of subtle program anomalies and the classification of program executions based on their behavioral characteristics. Our technique provides a powerful tool for improving program analysis and security.",
    "We propose a single neural probabilistic model based on variational autoencoder that can be conditioned on any arbitrary input. The model is based on a variational autoencoder (VAE), which is a generative model that learns to represent data in a latent space. The latent space is a low-dimensional representation of the data that captures the important features of the data. The VAE can be conditioned on any arbitrary input by simply adding the input to the decoder network. This allows the VAE to generate data that is specific to the input. The model is flexible and can be used for a variety of applications, such as image generation, text generation, and music generation.",
    "Variational Autoencoders (VAEs) were originally motivated as probabilistic generative models that learn a latent distribution and an approximate posterior distribution that is used to generate data. However, VAEs can also be used for representation learning, where the latent distribution is used to represent the data in a compressed form. In this paper, we propose a novel method for trading information between latents in hierarchical VAEs. Our method is based on the idea of using a hierarchical structure to model the latent distribution, where each level of the hierarchy represents a different level of abstraction. We show that our method is able to improve the representation learning performance of hierarchical VAEs and that it can be used to generate more diverse data.",
    "Understanding and characterizing the subspaces of adversarial examples aid in studying the robustness of deep neural networks. Local Intrinsic Dimensionality (LID) has been widely used to estimate the dimension of these subspaces. However, we show that LID is limited in capturing the complexity of adversarial subspaces, especially for high-dimensional data such as images. We propose a new measure, called Local Intrinsic Flatness (LIF), which provides a more accurate estimate of the subspace dimension. We demonstrate the effectiveness of LIF on various image datasets and show that it can better differentiate between adversarial and non-adversarial examples compared to LID.",
    "Generative adversarial networks (GANs) form a generative modeling approach known for producing appealing samples, but their training is notoriously unstable. This paper presents a variational inequality perspective on GANs, which provides new insights into their behavior and suggests novel training algorithms. We derive a variational inequality that characterizes the minimax objective of GANs, and show that this inequality is equivalent to a system of two coupled partial differential equations. We then propose a new training algorithm based on the proximal point method for solving variational inequalities, which we show to be more stable and efficient than existing GAN training algorithms. Finally, we demonstrate the effectiveness of our approach on a variety of image generation tasks.",
    "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, these methods typically ignore the importance of nodes and edges, leading to suboptimal performance. This paper proposes a novel graph neural network model, Predict then Propagate (P2P), that leverages personalized PageRank to capture node and edge importance. P2P consists of two phases: prediction and propagation. In the prediction phase, a node embedding model predicts the importance of nodes and edges. In the propagation phase, the importance is propagated through the graph to enhance node representations. P2P is simple to implement, efficient to train, and achieves state-of-the-art performance on various semi-supervised graph classification tasks.",
    "Obfuscated gradients, a type of gradient masking, emerge as a threat to adversarial defenses. We demonstrate their effectiveness in bypassing defenses and creating adversarial examples with improved transferability and robustness. Our findings highlight the need for more comprehensive defense mechanisms that can effectively handle obfuscated gradients and enhance the security of machine learning models.",
    "Methods that learn representations of nodes in a graph play a critical role in network analysis, visualization, and other downstream tasks. However, most existing methods are either supervised or semi-supervised, which require labeled data or make strong assumptions about the graph structure. In this work, we propose a novel deep Gaussian embedding method for graph representation learning that is completely unsupervised and inductive. Our method learns a low-dimensional Gaussian embedding of the nodes in the graph, such that the pairwise distances between the embedded nodes are consistent with the pairwise similarities between the original nodes. We show that our method can be used for a variety of tasks, including node classification, link prediction, and graph clustering, and achieves state-of-the-art results on several benchmark datasets.",
    "**Spherical CNNs: Extending Convolutional Neural Networks to Non-Euclidean Data**\n\nConvolutional Neural Networks (CNNs) have revolutionized the field of computer vision, enabling significant advancements in tasks such as image recognition and classification. However, CNNs are primarily designed to handle Euclidean data, which does not accurately represent many real-world scenarios.\n\nSpherical CNNs address this limitation by extending CNNs to non-Euclidean data, particularly spherical data. These networks utilize specialized convolutional layers that account for the curved geometry of spheres, ensuring effective feature extraction and classification.\n\nThis work provides an overview of Spherical CNNs, highlighting their advantages and applications in various domains. It discusses the challenges of working with non-Euclidean data and how Spherical CNNs overcome these challenges. The abstract concludes by emphasizing the potential of Spherical CNNs to unlock new possibilities in the field of deep learning and artificial intelligence.",
    "This paper presents a novel approach for classification using natural language processing (NLP) methods. The approach, termed Learning to SMILE(S), starts by extracting features from text data using NLP techniques. These features are then used to train a classifier, which can be used to predict the class of new text data. The approach is evaluated on a variety of text classification tasks, and is shown to achieve state-of-the-art results on several of these tasks.",
    "**Abstract**\n\nThe integration of Computer Vision (CV) and Deep Learning (DL) in agriculture is revolutionizing post-harvest handling by enabling the efficient detection of apple defects. This study presents a novel apple defect detection system using DL-based object detection. The proposed system utilizes a large dataset of apple images with meticulously annotated defects to train a robust deep learning model. The model achieves state-of-the-art performance in detecting various apple defects, including bruising, skin breaks, decay, and insect damage. By deploying this system in post-harvest handling facilities, growers and packers can significantly reduce product loss, improve product quality, and enhance customer satisfaction. The proposed system offers a significant leap forward in precision agriculture, paving the way for automated quality control and efficient resource utilization.",
    "Factorization techniques are presented for accelerating the training of recurrent neural networks with long short-term memory (LSTM) units. Two specific factorization schemes are proposed leading to (1) efficient application of the forget gate bias and (2) efficient computation of the candidate values. The proposed factorization schemes not only reduce the number of parameters but also make training faster. Compared to standard LSTM implementations, the proposed schemes can achieve up to 12.5% reduction in the number of parameters and up to 15% acceleration in the training time while preserving the classification performance.",
    "**Abstract:**\n\nState-of-the-art deep reading comprehension models primarily rely on recurrent neural networks (RNNs). However, RNNs suffer from sequential processing, which limits their ability to efficiently handle complex and long-range dependencies. This study proposes a novel approach using convolutional neural networks (ConvNets), known for their parallel processing capabilities, to address this limitation. The proposed model leverages ConvNets for feature extraction and attention mechanisms for context understanding, resulting in improved reading comprehension performance. Experimental evaluations demonstrate the effectiveness of the proposed approach, achieving state-of-the-art results on standard reading comprehension benchmarks.",
    "In this work, we analyze the reinstatement mechanism introduced by Ritter et al. (2018) to endow an agent with the ability to simulate past episodes in a goal-directed manner. We show that this mechanism leads to the emergence of neurons encoding abstract representations of past episodes and neurons encoding specific episodes. We study the impact of these neurons on the agent's performance in an episodic meta-reinforcement learning task and show that they contribute to efficient learning and adaptation to new tasks. Our findings shed light on the neural mechanisms underlying episodic memory and provide insights into the design of artificial agents with cognitive abilities.",
    "**Abstract**\n\nThe rate-distortion-perception function (RDPF) has emerged as a useful tool for assessing the perceptual quality of compressed video. This work presents a coding theorem for the RDPF that provides a theoretical foundation for the RDPF and allows for the design of efficient video codecs. The coding theorem shows that the RDPF can be decomposed into two components: a distortion component and a perception component. The distortion component is related to the amount of distortion introduced by the video codec, while the perception component is related to the perceptual quality of the compressed video. The coding theorem also shows that the RDPF is a continuous function of the distortion and perception components, which allows for the design of video codecs that can achieve any desired trade-off between distortion and perceptual quality.",
    "Neural Phrase-based Machine Translation (NPMT) explicitly models the phrase alignment between the source and target languages, allowing for more accurate and fluent translations. This approach combines the strengths of phrase-based translation with the power of neural networks, resulting in a more flexible and effective translation model.",
    "It is by now well-known that small adversarial perturbations can induce classification errors in deep, over-parameterized neural networks. Here, we approach the problem of defending against such adversarial examples by introducing sparse representations. We develop a new adversarial training algorithm, termed the adversarially-trained sparse network (ATSPNet), which yields networks whose internal representations are highly sparse during adversarial attacks. Using various network architectures and datasets, we show that ATSPNet significantly improves the robustness to adversarial attacks and achieves superior classification accuracy compared to methods based on adversarial training. By investigating network connectivity and generalization capabilities, we demonstrate that the sparse structure induced by ATSPNet not only enhances robustness but also improves generalization performance.",
    "We propose a new sample-efficient methodology, called Supervised Policy Update (SPU), for deep reinforcement learning. SPU leverages supervised learning to guide policy updates, significantly reducing the number of environment interactions required for training. By incorporating knowledge from past experiences, SPU enables agents to learn from both rewards and expert demonstrations, improving their performance and generalization capabilities. Experimental results demonstrate that SPU outperforms state-of-the-art reinforcement learning algorithms in various challenging environments, achieving higher reward and faster convergence with significantly fewer environment interactions.",
    "We present a parameterized synthetic dataset called Moving Symbols to support the objective study of unsupervised representation learning by video prediction models. Unlike real-world video, Moving Symbols can be generated to isolate specific representational challenges, such as the number and types of moving objects, the spatial relationships between objects, and the temporal complexity of object motion. Using Moving Symbols, we can comprehensively evaluate the representational capacity of different video prediction models and rigorously quantify their strengths and weaknesses. Our experiments show that Moving Symbols enables the identification of model-specific biases and limitations, and suggests directions for future research in unsupervised video representation learning.",
    "This work is a part of ICLR Reproducibility Challenge 2019, where we try to reproduce the Padam paper: \"Closing The Generalization Gap Of Adaptive Gradient Methods in Training Deep Neural Networks\". We reproduce the authors' experiments on CIFAR-10 and CIFAR-100 using PyTorch and compare our results to the original findings. We analyze the differences in performance and discuss potential reasons for the variations.",
    "Catastrophic forgetting (CF) is a fundamental problem in deep learning where models trained on new tasks forget previously learned ones. We present a comprehensive empirical study of CF in modern Deep Neural Networks (DNNs) using a large-scale benchmark. We analyze CF across different architectures, tasks, and training strategies. Our results provide insights into the mechanisms underlying CF and offer guidance for developing robust DNNs that can continually learn without forgetting.",
    "**Abstract:**\n\nDeep learning models for graphs have made significant strides in various domains. However, they remain vulnerable to adversarial attacks, which can degrade their performance. To address this challenge, this paper proposes a novel meta-learning-based approach to generate adversarial attacks on graph neural networks (GNNs). Our approach leverages a meta-learning framework to iteratively refine the attack strategy, leading to more effective and transferable adversarial perturbations. Experiments on real-world datasets demonstrate that our method significantly outperforms existing adversarial attack techniques, achieving high attack success rates with minimal perturbation. This work highlights the importance of meta-learning in enhancing the robustness of GNNs against adversarial attacks.",
    "Multi-Domain Learning (MDL) aims to train models that perform well across multiple domains, each with its own unique characteristics. This is achieved by minimizing the average risk across all domains, leading to models that generalize well to unseen domains. Multi-Domain Adversarial Learning (MDAL) is a novel approach to MDL that utilizes adversarial learning to encourage the model to learn domain-invariant features. By training the model to fool a domain discriminator while simultaneously minimizing the classification loss, MDAL promotes the extraction of features that are shared across domains. This approach has been shown to achieve superior performance in cross-domain tasks, demonstrating its effectiveness in learning from multiple domains.",
    "Unsupervised anomaly detection is challenging due to the lack of labeled anomaly data. Existing deep learning-based methods for unsupervised anomaly detection often rely on learning a low-dimensional representation of the normal data and then detecting anomalies as data points that deviate significantly from the learned representation. However, these methods are sensitive to noise and outliers, which can lead to false positives. In this paper, we propose a novel robust subspace recovery layer for unsupervised anomaly detection. The proposed layer is based on the idea of robust principal component analysis (RPCA), which can recover a low-dimensional subspace from data that is corrupted by noise and outliers. We show that the proposed layer can effectively suppress noise and outliers and learn a more robust representation of the normal data. This leads to improved performance on unsupervised anomaly detection tasks.",
    "Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex patterns in data. However, their predictions can be difficult to interpret, making it challenging to understand the underlying decision-making process. Hierarchical interpretations aim to address this issue by decomposing DNN predictions into a hierarchy of interpretable concepts. By providing a hierarchical structure for the interpretation, these methods make it easier to understand the relationships between different concepts and how they contribute to the overall prediction. In this abstract, we provide a brief survey of hierarchical interpretations for DNN predictions, discussing their different approaches and applications.",
    "In this work, we address the problem of musical timbre transfer, where the goal is to extract the timbre (sound color) from one audio signal (source) and transfer it to a different audio signal (target) while preserving the pitch and rhythm of the target. We propose a deep learning-based method called TimbreTron, which consists of a conditional generative adversarial network (CycleGAN) architecture that operates on the constant-Q transform (CQT) representation of the audio signals. The CQT representation allows for the disentanglement of the pitch and timbre components of the audio signals, making it suitable for timbre transfer. We evaluate the proposed method on a variety of musical instruments and demonstrate that TimbreTron can effectively transfer the timbre between different instruments while preserving the original pitch and rhythm.",
    "We propose a novel node embedding of directed graphs to statistical manifolds, which is based on a low-dimensional statistical manifold assumption. The embedding is obtained by minimizing the discrepancy between the empirical distribution of the graph and the distribution induced by the embedding. The resulting embedding preserves the local and global structures of the graph, and it can be used for various downstream tasks, such as node classification, link prediction, and community detection. Experiments on real-world datasets demonstrate the effectiveness of our approach compared to state-of-the-art methods.",
    "The impressive lifelong learning in animal brains is primarily enabled by plastic changes in synaptic strength. We propose a neuro-inspired deep learning framework, called Backpropamine, that efficiently optimizes the parameters of a plasticity-equipped neural network model. The plasticity component, termed Neuromodulated Differentiable Synapses (NDS), uses differentiable neuromodulation to adjust its parameters over time. To train the NDS network, we derive the gradients of the network's loss function with respect to its parameters. These gradients include terms that capture the effect of plasticity. We show that Backpropamine is amenable to efficient training in supervised, self-supervised, and continual learning scenarios. We demonstrate Backpropamine's effectiveness on several benchmark datasets, including CIFAR-10, CIFAR-100, ImageNet, and MNIST, where it achieves state-of-the-art performance in continual learning tasks.",
    "**Abstract:**\n\nEuclidean geometry has long dominated machine learning applications due to its simplicity and computational efficiency. However, it fails to capture the complex curvature often present in real-world data. This limitation has hindered the performance of many learning algorithms.\n\nTo address this issue, we propose Mixed-curvature Variational Autoencoders (MC-VAEs). MC-VAEs leverage the flexibility of Riemannian geometry to model data with arbitrary curvature. By incorporating Riemannian metrics into the VAE framework, we capture data that is not well-represented in Euclidean space.\n\nOur approach enables effective learning in scenarios where data exhibits non-Euclidean properties, such as manifolds or surfaces. Experiments demonstrate that MC-VAEs outperform traditional Euclidean VAEs on tasks involving data with curved geometry, including 3D shape reconstruction, image generation, and medical image analysis.\n\nThis work paves the way for more accurate and interpretable machine learning models by incorporating knowledge of data curvature.",
    "This paper explores methods for computing sentence representations from pre-trained word embeddings without any training, known as random encoders. We focus on sentence classification tasks, where the goal is to predict a label for a given sentence. We propose several simple and efficient random encoders and investigate their effectiveness on a range of benchmark datasets. Our experimental results show that random encoders can achieve competitive performance compared to more complex and computationally expensive methods, suggesting that they are a promising approach for sentence classification.",
    "Generative Adversarial Networks (GANs) are one of the most popular tools for learning complex high-dimensional distributions, but they often suffer from generalization and stability issues. In this work, we propose a novel approach to improving the generalization and stability of GANs by incorporating a novel loss function and a regularization term. The proposed loss function is based on a Wasserstein distance, which provides a more stable and meaningful measure of the distance between the generated and real data distributions. The regularization term encourages the generator to produce diverse and realistic samples, which helps to improve the generalization ability of the GAN. Experimental results on a variety of image generation tasks demonstrate that the proposed approach significantly improves the generalization and stability of GANs, leading to state-of-the-art results.",
    "**Abstract**\n\nIn this paper, we propose a novel model ensembling method based on the Wasserstein barycenter for multiclass and multilabel classification tasks. The Wasserstein barycenter provides a principled approach to find a central point in the probability simplex, which allows us to aggregate predictions from individual models effectively. Our method leverages the stability and robustness of the Wasserstein distance to handle the diversity and uncertainty among ensemble members. Empirical results on various real-world datasets demonstrate that our Wasserstein barycenter model ensembling approach outperforms existing ensembling methods, leading to improved classification accuracy and robustness.",
    "We present a method that learns to integrate temporal information, from a learned dynamics model, with a learned observation model, to predict the future interactions of multiple agents, from partial observations. We show that our method improves both the accuracy and the computational efficiency of predicting the future of a multi-agent system in several settings.",
    "**Abstract**\n\nModern neural networks are often over-parametrized, meaning that they have more weights and biases than necessary to achieve optimal performance. This over-parametrization can lead to instability in the training process and can make it difficult to interpret the network's learned weights.\n\nIn this paper, we propose a novel approach to equi-normalization of neural networks. Equi-normalization is a technique that scales the weights and biases of a network so that the mean and variance of the activation of each hidden unit are equal. This simple operation has a number of benefits: it improves the stability of the training process, it makes it easier to interpret the network's learned weights, and it can improve the network's generalization performance.\n\nWe evaluate our approach on a variety of tasks, including image classification, object detection, and natural language processing. Our results show that equi-normalization consistently improves the performance of neural networks, and that it is particularly effective for over-parametrized networks.",
    "Spherical data is found in many applications, including computer graphics, medical imaging, and remote sensing. However, traditional Convolutional Neural Networks (CNNs) are not well-suited for processing spherical data, as they do not respect the intrinsic symmetries of the sphere. In this paper, we propose DeepSphere, a novel equivariant graph-based spherical CNN that leverages the natural graph structure of the discretized sphere. DeepSphere consists of a series of equivariant graph convolutional layers, which are designed to preserve the symmetries of the sphere while learning powerful representations of spherical data. We evaluate DeepSphere on a variety of spherical classification and segmentation tasks, and demonstrate that it outperforms existing methods by a significant margin.",
    "We present graph wavelet neural network (GWNN), a novel graph convolutional neural network (CNN), leveraging graph wavelet transform (GWT) for extracting multi-scale features. Unlike traditional graph CNNs operating in spatial domain directly, GWNN operates in both spatial and frequency domains. By applying GWT, GWNN decomposes graph signals into multi-scale components, enabling feature extraction across different scales and frequencies. This design allows GWNN to capture both local and global structural information, leading to improved performance on graph-structured data. Experimental results demonstrate the superior performance of GWNN over state-of-the-art graph CNNs on various graph classification and regression tasks.",
    "A Variational Autoencoder model with the capacity to be conditioned on arbitrary data is introduced. This model combines the dimensionality reduction capabilities of a Variational Autoencoder with the flexibility of a conditioning mechanism. By leveraging a latent conditioning variable, the model can be tailored to specific conditioning data, enabling the generation of samples or the reconstruction of inputs under the influence of the conditioning information. This framework allows for flexible adaptation to various conditioning scenarios and paves the way for diverse applications in generative modeling, image manipulation, and data augmentation.",
    "We present the perceptor gradients algorithm -- a novel approach to learning symbolic representations based on the principle of perceptual grounding. The algorithm learns symbolic representations that are grounded in perceptual experiences by minimizing the perceptual reconstruction error, which measures the difference between the predicted and actual perceptual features of an object. The algorithm is evaluated on a variety of symbolic reasoning tasks, and is shown to outperform existing methods in terms of accuracy and generalization performance.",
    "We study the robustness to symmetric label noise of Graph Neural Networks (GNNs) training procedures. By combining the self-supervised meta-learning with the semi-supervised procedure, we propose a novel label noise-tolerant training method for GNNs. Through extensive experiments, we demonstrate that our method achieves superior performance compared to the existing methods and also outperforms the supervised baselines under supervised settings, indicating that our method can learn better representations.",
    "**Abstract:**\n\nThe recent use of \"Big Code\" with state-of-the-art deep learning methods offers promising avenues to improve software engineering tasks. This work investigates the use of Graph Neural Networks (GNNs) to infer JavaScript (JS) types from code. The proposed approach leverages the code structure and relationships between variables, functions, and classes represented as a graph. Our GNN model outperforms existing methods on several metrics, including type prediction accuracy and inference time. This work demonstrates the potential of GNNs for inferring JS types and opens up possibilities for incorporating context and relationships into type inference tasks.",
    "In this paper, we consider self-supervised representation learning to improve sample efficiency in reinforcement learning. We propose a novel approach that learns dynamics-aware embeddings by predicting future states from past observations. These embeddings encode the underlying dynamics of the environment and facilitate efficient decision-making. Our method outperforms existing self-supervised approaches on a variety of reinforcement learning tasks, demonstrating its effectiveness in improving sample efficiency.",
    "We study the problem of learning permutation invariant representations that can capture \"flexible\" notions of similarity. To this end, we propose a novel framework for representation learning with multisets, where the goal is to learn a function that maps a multiset to a vector that is invariant to the order of its elements. Our approach is based on the observation that the elements of a multiset can be viewed as the coefficients of a polynomial, and we propose to use a deep neural network to learn a mapping from polynomials to vectors. We show that our approach can be used to learn representations that are invariant to a variety of permutations, including permutations of the elements of a set, permutations of the rows of a matrix, and permutations of the channels of an image. We also show that our approach can be used to learn representations that are robust to noise and outliers.",
    "One way to interpret trained deep neural networks (DNNs) is by inspecting characteristics that neurons activate in response to different inputs. This method of interpretation has been used to understand the inner workings of DNNs and to gain insights into the underlying factors that influence their decisions. However, it can be challenging to identify which neurons are most important for a given task, and to explain why they activate in a certain way. In this paper, we propose a novel method for generating and automatically selecting explanations for DNNs. Our method uses a generative adversarial network (GAN) to generate a set of candidate explanations, and then uses a reinforcement learning algorithm to select the most relevant and informative explanations. We evaluate our method on a variety of DNNs and tasks, and show that it can generate high-quality explanations that are both accurate and informative.",
    "**Abstract**\n\nWe characterize the singular values of the linear transformation associated with a standard 2D multi-channel convolutional layer. We derive exact closed-form expressions for the singular values and show that they exhibit distinct behavior depending on the number of channels, filter size, and stride. Our analysis provides insights into the spectral properties of convolutional layers and their impact on network performance.",
    "**Learning to Represent Edits**\n\nWe introduce the problem of learning distributed representations of edits. By combining a \"neural editor\" with an autoencoder, we learn to represent edits as vectors that capture their semantic meaning. This allows us to perform a variety of tasks related to edit representation, such as clustering, retrieval, and generation. We demonstrate the effectiveness of our approach on a dataset of text edits, showing that our representations capture the semantics of edits and can be used to improve the performance of downstream tasks.",
    "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of Hamiltonian systems. SRNNs are derived from a discrete symplectic integrator, which preserves the symplectic structure of Hamiltonian systems. This allows SRNNs to learn the long-term behavior of Hamiltonian systems, even in the presence of noise and uncertainty. We demonstrate the effectiveness of SRNNs on a variety of tasks, including learning the dynamics of a pendulum, a double pendulum, and a robot arm.",
    "Spectral embedding is a popular technique for the representation of graph data. It involves constructing a low-dimensional embedding of the graph that preserves its spectral properties. Regularization techniques are often used to improve the quality of the embedding, particularly when the graph is noisy or incomplete. \n\nIn this paper, we consider the problem of spectral embedding of regularized block models. Block models are a class of random graphs that are used to model community structure in networks. We show that the spectral embedding of a regularized block model can be used to recover the community structure of the graph. \n\nWe also provide theoretical guarantees on the accuracy of the spectral embedding under mild assumptions on the regularization parameters. Our results have implications for the analysis of real-world networks, as many real-world networks can be well-approximated by block models.",
    "We study locality and compositionality in the context of learning representations for zero-shot learning (ZSL). ZSL is a challenging problem that requires models to classify images into unseen classes, using only features extracted from images of seen classes. Locality refers to the idea that similar images should be represented similarly, and compositionality refers to the idea that the representation of a complex object should be predictable from the representations of its parts. We propose a new representation learning method called LCC (Locality and Compositionality for ZSL) that explicitly enforces locality and compositionality. LCC learns a set of local features that capture the appearance of different parts of objects, and then uses a compositional model to combine these features into a representation of the entire object. We show that LCC achieves state-of-the-art results on several ZSL benchmarks, and that it is able to learn representations that are both local and compositional.",
    "We consider training machine learning models that are fair in the sense that their performance is as good for individuals from all sensitive groups as it is for individuals from the majority group. We propose a new approach to training fair models that is based on the idea of sensitive subspace robustness. Our approach involves training a model on a dataset that has been augmented with synthetic data that is designed to be challenging for the model to classify correctly. The synthetic data is generated by adding noise to the data in the sensitive subspace, which is the subspace of the data that contains information about the sensitive attribute. Our approach can be used to train fair models for a variety of machine learning tasks, including classification, regression, and ranking. We show that our approach outperforms existing approaches to training fair models on a variety of datasets.",
    "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, they typically require a large amount of labeled data for training, which can be expensive or difficult to obtain. To address this, we propose Predict then Propagate (PTP), a novel graph neural network framework that utilizes personalized PageRank to propagate labels from a few labeled nodes to unlabeled nodes. PTP consists of two main steps: (1) predicting the personalized PageRank scores of unlabeled nodes using a graph neural network; (2) propagating labels from labeled nodes to unlabeled nodes based on their predicted personalized PageRank scores. Extensive experiments on various benchmark datasets demonstrate that PTP significantly outperforms existing semi-supervised graph neural networks, especially when the amount of labeled data is limited.",
    "**Regularization Matters in Policy Optimization**\n\nDeep Reinforcement Learning (Deep RL) has been receiving increasingly more attention thanks to its encouraging successes in a variety of challenging domains, but issues caused by its high variance remain. In this paper, we highlight the paramount importance of regularization in policy optimization, shedding light on its formidable impact in reducing variance and enhancing stability. Our comprehensive investigation across a diverse range of Deep RL algorithms and environments reveals the profound benefits of regularization. Moreover, our theoretical analysis unveils the fundamental mechanisms underlying the effectiveness of regularization and establishes its crucial role in mitigating the challenges inherent in Deep RL.",
    "We consider a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss. We show that the loss landscape of such networks exhibits a number of desirable properties. First, the loss landscape is smooth and has no sharp transitions or local minima. Second, the loss landscape has a unique global minimum. Third, the loss landscape is relatively flat around the global minimum, which makes it easy to train the network to a good solution. These properties suggest that this class of deep neural networks is well-behaved and easy to train.",
    "Abstract:\n\nThis paper presents a theoretical framework for understanding deep locally connected ReLU networks. We first study the theoretical properties of deep and locally connected nonlinear networks, such as deep convolutional neural networks. We show that these networks can be approximated by a series of linear transformations followed by a nonlinear activation function. This allows us to analyze the convergence properties of these networks and to derive bounds on their generalization error. We then develop a new theoretical framework for deep locally connected ReLU networks. We show that these networks can be approximated by a series of locally connected linear transformations followed by a nonlinear activation function. This allows us to analyze the convergence properties of these networks and to derive bounds on their generalization error. We also show that deep locally connected ReLU networks can be used to represent a wide variety of functions, including complex and highly nonlinear functions. This makes them a powerful tool for machine learning tasks.",
    "Generative adversarial networks (GANs) have shown promising results in modeling the complex high-dimensional distributions of real-world data. However, their effectiveness in anomaly detection has been limited due to the difficulty in distinguishing between genuine and adversarial examples. This paper presents a novel GAN-based anomaly detection method that effectively discriminates between normal and anomalous data points. Our approach exploits the inherent adversarial nature of GANs to learn a discriminative representation that captures the essential characteristics of normal data. We introduce a new loss function that encourages the generator to produce diverse and realistic samples while penalizing the discriminator for misclassifying real data as anomalous. Experimental results on various datasets demonstrate the superiority of our method over existing GAN-based and non-GAN-based anomaly detection techniques. Our approach achieves state-of-the-art performance in detecting both global and local anomalies, showcasing its effectiveness in practical applications.",
    "Most state-of-the-art neural machine translation systems, despite being different in architectural skeletons (e.g. recurrence, convolutional), are largely based on a sequence-to-sequence framework which models the conditional probability of target sentence y given source sentence x and is trained to minimize the cross-entropy objective. This paper presents a novel attention model, namely Phrase-Based Attention (PBA), where the probability is factorized into two steps: model the conditional probability of target phrase \\(y_i\\) given source phrase \\(x_j \\) conditioned on a context vector, and then model the probability of combination given the target phrase sequence \\(y\\) and the source phrase sequence \\(x\\). We evaluate PBA on a variety of translation tasks and find that it outperforms the strong baselines.",
    "We propose an algorithm combining calibrated prediction and generalization bounds from learning theory to construct PAC confidence sets for deep neural networks (DNNs). Our algorithm achieves state-of-the-art performance on calibration and PAC confidence set accuracy on various DNN architectures and datasets, including ImageNet. Our approach is theoretically sound and computationally efficient, making it a promising tool for improving the reliability and interpretability of DNNs.",
    "The rate-distortion-perception function (RDPF) has emerged as a useful tool for characterizing the relationship between the rate, distortion, and perceptual quality of compressed images. In this paper, we present a coding theorem for the RDPF that provides a lower bound on the expected perceptual distortion of a compressed image. The theorem is based on the assumption that the perceptual distortion is a monotonically non-decreasing function of the rate and distortion. We show that the lower bound is tight for a wide range of RDPFs, including those that are commonly used in practice. Our results provide a theoretical foundation for the use of the RDPF in image compression.",
    "Inspired by natural language processing, we address graph classification using a novel variational recurrent neural network (VRNN) model. Graphs are ubiquitous data structures but require tailored neural architectures to process their structural information. Our model excels at this task by employing a recurrent neural network with a variational learning objective. This objective enables the model to capture the uncertainty in graph structure, leading to improved classification accuracy.",
    "Neural network pruning techniques can dramatically reduce the size of trained models, making them more efficient and easier to deploy. This paper presents a novel lottery ticket hypothesis, which states that a winning lottery ticket\u2014a small, randomly initialized subnetwork\u2014exists within every large, randomly initialized network. This subnetwork can be trained to achieve the same accuracy as the full network, even if the subnetwork is trained on a much smaller dataset. The lottery ticket hypothesis is supported by extensive experiments on a variety of datasets and network architectures. These experiments show that lottery ticket subnetworks can be found in both convolutional and recurrent networks, and that they can be trained to achieve state-of-the-art accuracy on a variety of tasks. The lottery ticket hypothesis provides a new understanding of the training process and suggests that neural networks may be much more efficient than previously thought.",
    "Generative adversarial networks (GANs) form a generative modeling approach known for producing appealing samples, but also for exhibiting instability during training due to vanishing gradients and mode collapse. We propose to view GANs from the perspective of variational inequalities (VIs). Specifically, we reformulate the adversarial minimax objective as a VI, and build upon recent advances in VI theory. We show that the reformulation leads to a more stable training process, improves sample quality, and provides insights into the behavior of GANs.",
    "Symplectic ODE-Net (SymODEN) is a deep learning framework that can infer Hamiltonian dynamics with control. SymODEN is based on a symplectic integrator, which is a numerical method for solving Hamiltonian systems that preserves the symplectic structure of the system. This makes SymODEN well-suited for learning Hamiltonian dynamics, as it can capture the conservation laws and symmetries of the system. SymODEN can be used to control Hamiltonian systems, as it can learn to predict the future state of the system given a control input. This makes SymODEN a powerful tool for applications such as robotics, where it can be used to control robots with complex dynamics.",
    "**Abstract:**\n\nGraph embedding techniques have gained prominence in various applications that require data representation in a low-dimensional space. However, designing scalable embedding methods that preserve the structural and semantic information of graphs remains challenging. This paper presents GraphZoom, a multi-level spectral approach for accurate and scalable graph embedding. GraphZoom leverages the spectral properties of graphs to construct hierarchical representations, extracting features from different frequency bands. These hierarchical representations are then combined using a novel aggregation scheme that enhances the embedding's expressiveness and robustness. Extensive experiments demonstrate GraphZoom's superior performance in terms of accuracy, scalability, and robustness compared to state-of-the-art methods on various graph classification, clustering, and visualization tasks.",
    "Distributed optimization is crucial for solving large-scale machine learning problems. However, stragglers, i.e., slow workers, can significantly degrade the performance of distributed optimization algorithms. To address this issue, we propose Anytime MiniBatch (AMB), a novel distributed optimization algorithm that exploits stragglers. AMB leverages a mini-batching strategy to reduce the impact of stragglers. It iteratively constructs mini-batches from the available workers and updates the model parameters based on the mini-batches. The algorithm ensures that the model parameters are always up-to-date, even in the presence of stragglers. We prove the convergence of AMB under mild assumptions. Experiments on real-world datasets demonstrate that AMB significantly outperforms existing distributed optimization algorithms in terms of both convergence speed and solution quality, especially when stragglers are present.",
    "Scaling end-to-end reinforcement learning to control real robots from vision presents a series of challenges, including the need for data-efficient learning, handling high-dimensional visual inputs, and adapting to changing and diverse environments. In this paper, we propose a decoupled approach to reinforcement learning that separates feature extraction from policy learning. This approach enables us to leverage advances in self-supervised representation learning for vision, while allowing for efficient policy learning using reinforcement learning. We evaluate our approach in a challenging goal-based robotic manipulation task, demonstrating that it significantly improves sample efficiency and generalization performance compared to end-to-end baselines. Our results suggest that decoupling feature extraction from policy learning is a promising approach for scaling reinforcement learning to real-world robotic applications.",
    "**Abstract:**\n\nReinforcement learning (RL) faces difficulties in discovering efficient strategies for complex tasks with sparse rewards. To address this, we explore the concept of the Information Bottleneck (IB) in the context of RL. The IB is a framework for maximizing the mutual information between a policy and an informative representation of the environment, while simultaneously minimizing the redundancy between the policy and the environment.\n\nBy leveraging the IB, our proposed approach, InfoBot, learns policies that efficiently capture the relevant information for successful task completion. Unlike conventional RL algorithms that directly optimize for reward, InfoBot focuses on transmitting and utilizing the most relevant information through its policy, thereby maximizing robustness to reward sparsity.\n\nOur experiments demonstrate that InfoBot outperforms baselines on various RL tasks, exhibiting superior reward performance and sample efficiency. This suggests that the IB principle offers a promising framework for advancing RL algorithms in challenging environments with limited reward feedback.",
    "Multilingual machine translation, which translates multiple languages with a single model, has attracted much attention. Most existing multilingual machine translation approaches first learn a multilingual encoder-decoder model and then apply it to individual language pairs. However, such a training strategy ignores the potential knowledge transfer between different language pairs. In this paper, we propose a novel approach to multilingual machine translation with knowledge distillation, which explicitly distills knowledge from a pre-trained multilingual teacher model to a student model for each individual language pair. Our approach can effectively leverage the knowledge learned from other language pairs, leading to improved translation quality. Experimental results on the WMT14 English-German, English-French, and English-Romanian translation tasks demonstrate the effectiveness of our approach.",
    "PyTorch Geometric is a library for deep learning on irregularly structured input data such as graphs and point clouds. It provides a comprehensive set of tools for graph representation learning, including data loaders, model architectures, and training algorithms. In this tutorial, we will introduce the basic concepts of graph representation learning and show how to use PyTorch Geometric to build and train graph neural networks.",
    "**Abstract:**\n\nVariational autoencoders (VAEs) are popular deep generative models, but their capabilities can be limited by various factors. This work aims to diagnose and enhance VAEs by identifying common challenges and developing effective solutions. Through empirical studies and theoretical analysis, we uncover issues related to model instability, latent disentanglement, and sample quality. Our proposed enhancements include regularization techniques, architectural modifications, and training strategies. Experiments demonstrate significant improvements in latent space stability, disentanglement, and the generation of higher-quality samples. These advancements contribute to a deeper understanding of VAEs and their practical applications in generative modeling.",
    "**Bridging Adversarial Robustness and Gradient Interpretability**\n\nAdversarial training, a technique designed to enhance model robustness against adversarial attacks, involves modifying the training process to consider potential adversarial perturbations. This study investigates the relationship between adversarial robustness and gradient interpretability in the context of this training scheme.\n\nOur findings reveal that adversarial training impacts the interpretability of gradients used in gradient-based explanation methods. Specifically, adversarial training can lead to changes in the distribution of gradients, potentially affecting the reliability of gradient-based explanations. This suggests a need to explore alternative explanation methods resilient to the effects of adversarial training.",
    "This is the proceedings of the Computer Vision for Agriculture (CV4A) Workshop that was held in conjunction with the International Conference on Learning Representations (ICLR) in April 2020. The workshop brought together researchers and practitioners from the fields of computer vision, agriculture, and robotics to discuss the latest advances and challenges in applying computer vision techniques to agricultural problems.",
    "Proceedings of the 1st AfricaNLP Workshop held on 26th April alongside ICLR 2020, Virtual Conference.",
    "In this work, we demonstrate preliminary findings of deep multi-task learning in histopathology for developing widely generalizable models. By utilizing multi-task learning, we aim to leverage shared knowledge across different tasks to enhance the generalization capabilities of individual models. We present results on a histopathology dataset, showcasing the effectiveness of our approach in improving model performance on tasks such as tissue classification, cell segmentation, and disease diagnosis. These findings suggest that multi-task learning holds promise for developing robust and generalizable models in the field of histopathology.",
    "**Abstract**\n\nCompositionality, the principle that complex concepts can be expressed by combining simpler units, is fundamental to human language. This study introduces a novel neural iterated learning model that captures this principle by allowing agents to iteratively expand their language. The model's internal representation of words and phrases reflects compositional structure, facilitating the emergence of novel composite expressions. Experiments demonstrate that the model achieves sustained compositional growth over extended training, capturing regularities in human languages, such as hierarchical structure and open-ended vocabulary. The findings suggest that compositional languages can arise from simple iterative learning processes, providing insights into the origins of human language.",
    "Text generation is a fundamental task in natural language processing (NLP), with applications in a wide range of tasks such as summarization, dialogue, and machine translation. Traditional text generation models typically rely on sequence-to-sequence architectures, which can be computationally expensive and difficult to train. In this paper, we propose a novel Residual Energy-Based Model (REB) for text generation. REB is a non-autoregressive model that generates text by optimizing an energy function over the entire sequence. The energy function is defined as a sum of residual energies, which are computed from the difference between the predicted and target tokens. REB is trained using a contrastive loss, which encourages the model to generate sequences that are similar to the target sequences. Experimental results on three text generation tasks show that REB achieves state-of-the-art performance while being more efficient and easier to train than autoregressive models.",
    "**Abstract**\n\nThis paper proposes an energy-based model (EBM) of protein conformations that operates at atomic scale. The EBM is based on a novel energy function that is designed to capture the physical interactions that govern protein folding. The model is able to predict the conformations of small proteins with high accuracy, and it can also be used to study the dynamics of protein folding. The EBM is a promising tool for understanding the structure and function of proteins.",
    "**Abstract**\n\nWe demonstrate that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel are equivalent. This result implies that both kernels induce the same function space for continuous functions. Consequently, any deep neural network with randomly initialized weights and ReLU activations can be expressed as a linear combination of a set of Laplace kernel functions. This finding provides a theoretical foundation for the remarkable generalization performance of deep neural networks and establishes a connection between deep learning and kernel methods.",
    "We propose a novel node embedding of directed graphs to statistical manifolds, which is based on a local tangent space alignment. Our method preserves not only the local geometric structure but also the global topology of the graph. We demonstrate the effectiveness of our method on a variety of tasks, including link prediction, node classification, and graph clustering.",
    "**Abstract:**\n\nEuclidean geometry has been the foundation for many machine learning applications, but it has limitations in representing complex data. Mixed-Curvature Variational Autoencoders (MC-VAEs) overcome this by introducing Riemannian geometry, allowing for more flexible and expressive data representation. Euclidean VAEs are limited to a linear latent space, while MC-VAEs can learn arbitrary curved manifolds. This enhanced modeling power enables MC-VAEs to capture complex relationships within data, leading to improved performance in areas such as image generation, natural language processing, and manifold learning.",
    "This study introduces exact convex regularizers for training two- and three-layer Convolutional Neural Networks (CNNs) with ReLU activations. The proposed regularizers are shown to be convex and provide a polynomial-time training algorithm for these networks. The study provides a theoretical foundation for the use of ReLU activations in CNNs and demonstrates the effectiveness of the proposed regularizers on a variety of image classification tasks.",
    "We propose a new metric space of ReLU activation codes equipped with a truncated Hamming distance specifically designed for rating the quality of networks. Specifically, we embed ReLU networks into this metric space, respecting the topology of the network. We show that the quality of a network, in terms of its generalization performance, is strongly correlated with the location of its embedding in this metric space. Our approach outperforms accuracy by 13% on MNIST and by 20% on CIFAR-10 when used to compare different models for a given task. We also demonstrate that our code space can be used to interpolate between different networks in a continuous fashion, providing new insights into the optimization landscape of neural networks.",
    "This paper introduces the first dataset of satellite images labeled with forage quality by on-the-ground measurements in northern Kenya, a region heavily affected by drought and where pastoralists rely heavily on livestock for their livelihoods. The dataset consists of 10,000 satellite images collected over three years, each labeled with the corresponding forage quality measurements collected on the same day at the same location. The dataset is publicly available and will be used to develop machine learning models for predicting forage quality from satellite imagery. These models will help pastoralists to make better decisions about when and where to graze their livestock, leading to increased livestock production and improved livelihoods for pastoralists in northern Kenya.",
    "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer. The layer utilizes the low-rank property of normal data to learn a subspace representation of the data, which is robust to noise and outliers. This allows the network to detect anomalies that deviate significantly from the subspace, even in the presence of noise or missing data. We demonstrate the effectiveness of our approach on several benchmark datasets, and show that it outperforms state-of-the-art unsupervised anomaly detection methods.",
    "**Abstract**\n\n**Lifelong learning** requires **plastic changes** that adapt to changing environments. In animal brains, these changes are primarily observed at synapses, the connections between neurons. Inspired by neuromodulated plasticity in biological synapses, we introduce **Backpropamine**, a framework for training self-modifying neural networks using **differentiable neuromodulation**. This allows networks to dynamically regulate their own synaptic plasticity during training, enabling more efficient and adaptive learning. Empirical evaluations show that networks trained with Backpropamine significantly **outperform** standard training algorithms on a variety of lifelong learning tasks, including continual learning, reinforcement learning, and unsupervised representation learning.",
    "**Abstract:**\n\nThe inclusion of Computer Vision and Deep Learning technologies in Agriculture aims to increase the efficiency and quality of post-harvest handling processes. This paper presents an innovative approach for apple defect detection using Deep Learning-based object detection. The proposed method utilizes a pre-trained Convolutional Neural Network (CNN) architecture to identify and classify various apple defects, including bruises, scratches, and blemishes. By leveraging the capabilities of Deep Learning, the model achieves high accuracy in detecting defects, even in challenging lighting and background conditions. This technology has the potential to enhance post-harvest handling practices by enabling automated sorting, grading, and quality control, ultimately reducing food waste and improving the overall profitability of the apple industry.",
    "**Abstract**\n\nRecent breakthroughs in neural machine translation (NMT) have revolutionized the field, producing impressive results for major European languages. However, similar advancements have been lacking for African languages, including those spoken in South Africa. This study aims to address this disparity by applying NMT techniques to the official languages of South Africa, unlocking the potential for improved communication and access to information.",
    "We propose an algorithm combining calibrated prediction and generalization bounds from learning theory to construct, for any deep neural network classifier, a set of confidence scores such that, with high probability, the true class of each input is included in the set. Our algorithm runs in polynomial time and does not require any knowledge of the network's internal parameters. We evaluate our algorithm on a variety of datasets and find that it consistently outperforms existing methods.",
    "Pre-trained language models (LMs) have demonstrated impressive performance in a wide range of natural language processing tasks. However, it remains unclear to what extent these models are aware of linguistic phenomena such as phrases. In this paper, we conduct a series of experiments to investigate the ability of pre-trained LMs to identify and manipulate phrases. Our results show that LMs are indeed aware of phrases, and that they can be used to perform a variety of phrase-related tasks, such as phrase induction, phrase paraphrase, and phrase grammaticality judgment. We also show that simple baselines perform surprisingly well on these tasks, suggesting that the ability to represent phrases is a fundamental property of language models.",
    "Magnitude-based pruning is one of the simplest methods for pruning neural networks. Despite its simplicity, it achieves state-of-the-art results on various vision tasks. However, magnitude-based pruning suffers from several limitations. First, it can result in an uneven distribution of pruned weights, with some neurons pruned heavily while others remain largely intact. Second, it can lead to the pruning of important weights, which can degrade the performance of the network.\n\nIn this paper, we propose a new pruning method called Lookahead that addresses the limitations of magnitude-based pruning. Lookahead uses a novel pruning criterion that takes into account the future impact of pruning on the network's performance. This allows Lookahead to prune weights more evenly and to avoid pruning important weights. As a result, Lookahead achieves state-of-the-art results on various vision tasks, while reducing the computational cost of pruning.",
    "**Abstract:**\n\nAs the share of renewable energy sources in the present electric energy mix rises, their inherent fluctuations pose challenges to grid stability. This study proposes a reinforcement learning approach to optimize renewable electricity consumption. The approach leverages a Q-learning algorithm to learn the optimal charging/discharging schedule of an electric vehicle fleet considering real-time renewable generation and electricity prices. Results demonstrate that the proposed method effectively reduces the reliance on fossil fuel generation, enhances renewable energy utilization, and mitigates grid imbalances caused by renewable variability.",
    "We report our experiments in building a domain-specific Tigrinya-to-English neural machine translation system for humanitarian response. We use transfer learning from a pre-trained English-Tigrinya neural machine translation model and fine-tune it on a parallel corpus of Tigrinya and English texts related to humanitarian aid. Our results show that transfer learning can significantly improve the quality of the Tigrinya-to-English translation, achieving a BLEU score of 28.4 on a held-out test set. We also conduct human evaluation experiments and find that our system produces translations that are of high quality and suitable for use in humanitarian contexts.",
    "**Abstract**\n\nNigerian Pidgin is a widely spoken language in Nigeria, with several variants across the country. This study aims to establish supervised and unsupervised neural machine translation (NMT) baselines for Nigerian Pidgin. We present a detailed analysis of the collected data, pre-processing techniques, and experimental setup. Our results demonstrate the feasibility of NMT for Nigerian Pidgin, achieving promising translation quality for both supervised and unsupervised approaches. This work provides a foundation for further research on NMT for Nigerian Pidgin, facilitating communication and language technology development for this important language.",
    "Estimating grape yield prior to harvest is important to commercial vineyard production as it informs crop load management, labor needs, and marketing decisions. In this paper, we propose a novel approach for estimating grape yield on the vine using multiple images. Our approach leverages deep learning techniques to extract features from the images and build a regression model that predicts grape yield. We evaluate our approach on a dataset of grapevine images and demonstrate its effectiveness in accurately estimating grape yield. The proposed approach has the potential to improve the efficiency and accuracy of yield estimation in commercial vineyards.",
    "**Abstract:**\n\nAutomatic change detection and disaster damage assessment are currently procedures requiring a huge amount of human effort for the identification and surveying of the damaged areas. This process is time-consuming for large-scale scenarios and can delay response efforts in emergency situations. In this paper, a novel method for damage assessment in satellite imagery is presented. The approach leverages multi-temporal fusion of satellite imagery to analyze changes in the built environment. Damage is identified by analyzing the discrepancies between pre- and post-event imagery, considering changes in spectral information, texture, and context. The method has been evaluated using a dataset of satellite imagery from areas affected by natural disasters, demonstrating promising results for rapid and accurate damage detection.",
    "**How Chaotic Are Recurrent Neural Networks?**\n\nRecurrent neural networks (RNNs) are non-linear dynamic systems, and their chaotic behavior has been a concern. Previous work believes that RNN may suffer from chaotic attractors, which can prevent the network from learning and make it unstable. In this paper, we investigate the chaotic behavior of RNNs and show that, while RNNs can exhibit chaotic behavior, this behavior is not as prevalent as previously believed. We also show that the degree of chaotic behavior can be controlled by the network's parameters, and that RNNs can be trained to avoid chaotic behavior.",
    "Fine-tuning a pretrained BERT model is the state of the art method for extractive/abstractive text summarization. In this work, we investigate the fine-tuning of BERT for Arabic text summarization. We experiment with different hyper-parameters and training data, and we compare our results to the current state of the art. We find that our model achieves competitive results on the Arabic Summarization Dataset.",
    "**Abstract**\n\nCluster analysis is a valuable tool for identifying patterns in residential energy consumption data. However, determining the optimal number of clusters can be challenging. This study proposes using competency questions to guide the selection of clustering structures. Domain experts and visual analysis were employed to identify preliminary clustering structures, which were then evaluated using a set of competency questions that assessed the comprehensibility, granularity, and usefulness of the clusters. The results suggest that competency questions can be an effective approach to selecting optimal clustering structures for residential energy consumption patterns. The proposed method provides a systematic and objective approach to cluster selection, reducing reliance on subjective criteria and improving the interpretability and applicability of the clustering results.",
    "**Reinforcement Learning with Random Delays**\n\n**Abstract:**\n\nAction and observation delays are prevalent in Reinforcement Learning (RL) applications, particularly those involving remote control. These delays introduce challenges for traditional RL methods, which assume instantaneous actions and observations. To address this issue, this paper explores RL with random delays.\n\nWe propose a novel algorithm that incorporates delay estimation and recovery mechanisms. The algorithm dynamically adapts to varying delay distributions, enhancing training stability and performance. Experimental results demonstrate that our approach outperforms existing methods in scenarios with both fixed and random delays.\n\nOur work advances the field of RL by providing a practical solution for handling random delays, enabling the adoption of RL in applications where delays are unavoidable.",
    "We demonstrate that differentially private machine learning has not yet reached its \"AlexNet moment\" on ImageNet. When trained on the full ImageNet dataset, differentially private models fall dramatically short of the performance of their non-private counterparts. Even with simple models, the gap remains substantial. We show that adding more data can partially close the gap, but that a greater understanding of differentially private features is also necessary.",
    "**Abstract**\n\nWe introduce Symplectic ODE-Net (SymODEN), a novel deep learning framework for modeling Hamiltonian dynamics with control inputs. SymODEN leverages symplectic integrators, which preserve the energy and phase-space volume of the underlying Hamiltonian system, to construct a neural network architecture. This symplectic structure enables the network to accurately capture the dynamics of controlled Hamiltonian systems, even with large control inputs. Our experimental results demonstrate the effectiveness of SymODEN in modeling a variety of Hamiltonian systems, including a nonlinear pendulum, a robotic arm, and a quadrotor.",
    "We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of symplectic systems, which arise in various scientific domains such as celestial mechanics and molecular dynamics. SRNNs are designed to preserve the symplectic structure of the underlying system, leading to improved accuracy and stability in modeling and predicting its behavior. We present theoretical analysis and empirical results demonstrating the advantages of SRNNs over conventional recurrent neural networks in capturing the intricate dynamics of symplectic systems.",
    "Anomaly detection is finding patterns that substantially deviate from those seen previously, is one of the most important problems in machine learning. In this paper, we propose a novel classification-based anomaly detection algorithm that can be applied to general data. Our algorithm first trains a classifier to distinguish between normal and anomalous data. Then, it uses the classifier to score new data points, and those with the highest scores are considered anomalies. We evaluate our algorithm on a variety of datasets, and show that it outperforms existing anomaly detection algorithms in terms of both accuracy and efficiency.",
    "We consider training machine learning models that are fair in the sense that their performance is independent of a sensitive attribute. This requires satisfying two properties. First, the model must be individually fair: its predictions should be independent of the sensitive attribute for each individual. Second, the model must be sensitive subspace robust: the model's predictions should be insensitive to small changes in the sensitive subspace, which is the subspace spanned by the sensitive attribute and any other features that are correlated with it. We propose a method for training models that satisfy both of these properties. Our method uses a regularization term that encourages the model to make predictions that are invariant to changes in the sensitive subspace. We show that our method can significantly improve the fairness of machine learning models, without sacrificing their accuracy.",
    "This paper explores self-supervised representation learning for improving sample efficiency in reinforcement learning. We propose a novel approach, Dynamics-aware Embeddings, which leverages the dynamics of the environment to learn representations that are informative for decision-making. Our method learns a mapping from observations to a latent space, where the latent representations encode information about the dynamics of the environment. This allows the agent to make decisions based on a richer understanding of the environment, leading to improved sample efficiency. We evaluate our approach in a range of reinforcement learning tasks and demonstrate significant improvements over baselines.",
    "**Abstract**\n\nIn this paper, we propose SenSeI, a novel approach for enforcing individual fairness in machine learning models. We cast fair machine learning as invariant machine learning, formulating it as a problem of enforcing sensitive set invariance. We introduce a new notion of sensitive set invariance, which requires the model's predictions to be invariant to membership in a protected group while allowing for variation based on other relevant features. We develop an efficient optimization algorithm for training models that satisfy sensitive set invariance. Experimental results on multiple datasets show that SenSeI effectively enforces individual fairness while maintaining model accuracy.",
    "**Graph-Based Continual Learning**\n\nDespite significant advances, continual learning models still suffer from catastrophic forgetting when exposed to incrementally learned data. This limitation hinders the practical applicability of such models in real-world scenarios. To address this challenge, we propose a novel graph-based continual learning approach that leverages graph neural networks (GNNs) to capture the relationships between data points. Our approach utilizes a knowledge graph to store the learned information from previous tasks and guides the learning process of new tasks. By maintaining connections between related concepts and tasks, our model can effectively prevent forgetting and facilitate knowledge transfer. Experimental results on benchmark datasets demonstrate the superior performance of our approach compared to existing continual learning methods.",
    "This paper introduces a novel self-attention formulation for computer vision tasks. The proposed approach, called Group Equivariant Stand-Alone Self-Attention (GESSA), imposes group equivariance to arbitrary symmetry groups, providing several advantages over existing self-attention mechanisms. By enforcing equivariance, GESSA ensures that the attention maps respect the underlying symmetries of the input data, leading to improved performance on tasks such as object recognition and image segmentation. Furthermore, GESSA is designed as a stand-alone module, making it easy to integrate into existing deep learning architectures. Experimental results on various datasets demonstrate the effectiveness of GESSA, achieving state-of-the-art results on several challenging tasks.",
    "We propose studying few-shot graph classification in graph neural networks. We introduce a novel approach based on graph spectral measures to construct super-classes that group similar graphs, with super-classes determined by the spectral properties of graphs and their corresponding super-class assignment determined automatically from the structural similarity between graphs. The proposed method addresses the high variance and often insufficient support in the training set of few-shot learning and enables rapid adaptation to unseen classes. Experiments demonstrate the effectiveness of our method on various graph classification datasets, achieving state-of-the-art results in few-shot graph classification.",
    "We investigate the positional encoding methods used in language pre-training models such as BERT. We show that the standard sinusoidal positional encoding, while effective, has limitations: it is not permutation-equivariant, making it sensitive to the order of tokens in the input sequence. We propose a novel permutation-equivariant positional encoding that addresses this limitation. Our encoding is based on a learnable distance matrix between tokens, which captures the relative distances between tokens in the sequence. We demonstrate the effectiveness of our encoding in a variety of language pre-training tasks, showing that it consistently outperforms sinusoidal positional encoding.",
    "Graph embedding techniques have been increasingly deployed in a multitude of different applications that involve complex network data. GraphZoom is a novel graph embedding method that utilizes a multi-level spectral approach to generate accurate and scalable embeddings. Notably, GraphZoom leverages the spectral properties of graphs to construct a hierarchy of graph representations, where each level captures distinct patterns and scales of the data. This hierarchical structure enables GraphZoom to effectively capture local and global information, resulting in embeddings that are more comprehensive and discriminative. Furthermore, GraphZoom is highly scalable, allowing it to handle graphs of varying sizes and complexities. Empirical evaluations demonstrate that GraphZoom outperforms state-of-the-art graph embedding methods across a range of tasks, including node classification, link prediction, and graph classification.",
    "**DDPNOpt: Differential Dynamic Programming Neural Optimizer**\n\nTraining Deep Neural Networks (DNNs) can be formulated as an optimal control problem. The objective is to minimize a loss function over a horizon of training steps, while respecting the non-linear dynamics of the network. Traditional optimization algorithms often struggle with this problem due to the high dimensionality and non-convexity of the loss landscape.\n\nWe propose DDPNOpt, a novel optimization algorithm that leverages differential dynamic programming (DDP) to efficiently solve this optimal control problem. DDP decomposes the problem into a sequence of subproblems, each of which is solved using a neural network. This approach allows us to exploit the structure of the problem and learn an optimal policy for training the network.\n\nExperimental results on a variety of benchmark datasets show that DDPNOpt significantly improves the convergence speed and generalization performance of DNNs compared to existing optimization algorithms. Our approach is particularly well-suited for large-scale and resource-constrained settings, where efficient optimization is crucial.",
    "In this paper, we investigate the effects of releasing arXiv preprints of papers that are under double-blind review. We find that the release of preprints can lead to the de-anonymization of authors, which can have a number of negative consequences, including bias in the review process and retaliation against authors. We recommend that authors carefully consider the risks and benefits of releasing preprints before doing so.",
    "**Abstract**\n\nReinforcement learning (RL) has achieved impressive performance in a variety of online settings in which agents learn from experience through trial and error. However, offline RL, where agents must learn from a fixed dataset of experience without further interaction with the environment, remains a significant challenge. In this paper, we propose OPAL, a novel offline primitive discovery algorithm that leverages the structure of the environment to identify reusable subroutines, or primitives, that can accelerate learning. OPAL is trained offline on a dataset of expert demonstrations and uses a hierarchical clustering algorithm to identify a set of primitives that cover a diverse range of behaviors. By leveraging these primitives, OPAL enables agents to learn complex tasks more efficiently and effectively. We evaluate OPAL on a suite of challenging offline RL benchmarks and show that it significantly outperforms existing offline RL algorithms while achieving competitive performance with online RL methods.",
    "Stochastic Gradient Descent (SGD) and its variants are mainstream methods for training deep networks in machine learning. However, despite their widespread use, there is still a lack of understanding of their convergence behavior. In this paper, we develop a diffusion theory for SGD and show that it exponentially favors flat minima. This theoretical result provides a fundamental explanation for the observed behavior of SGD in practice, and it can also guide the design of new training algorithms that are more robust to flat minima.",
    "**Spectral Embedding of Regularized Block Models**\n\nSpectral embedding is a popular technique for representing graph data. However, it can be sensitive to noise and outliers. Regularization techniques can help to mitigate these effects.\n\nIn this paper, we develop a new spectral embedding method that is based on a regularized block model. The block model assumes that the graph is composed of a number of blocks, each of which is densely connected. The regularization term encourages the embedding to respect the block structure of the graph.\n\nWe show that our new method outperforms existing spectral embedding methods on a variety of datasets. Our method is particularly effective on graphs with noise and outliers.",
    "In this work, we study the locality and compositionality of representation learning for zero-shot learning (ZSL). We investigate the hypothesis that learning local and compositional representations can improve the performance of ZSL models. We propose a novel architecture, called the Locality and Compositionality Network (LCN), which explicitly learns local and compositional representations. Our experiments show that the LCN significantly outperforms state-of-the-art ZSL models on both seen and unseen classes.",
    "**Abstract**\n\nWe study the problem of learning permutation invariant representations that can capture \"flexible\" notions of similarity. In contrast to standard vector-based representations, which are not permutation invariant, we propose to represent objects as multisets of vectors, which allows for a more expressive encoding of flexible similarity. We develop a novel representation learning framework that maps multisets to a vector space, preserving their permutation invariance and capturing their intrinsic structure. Experiments on various computer vision tasks demonstrate the effectiveness of our approach in learning discriminative representations and achieving state-of-the-art results.",
    "Deep Reinforcement Learning (Deep RL) has been receiving increasingly more attention thanks to its encouraging results on complex sequential decision-making tasks. However, Deep RL algorithms are often unstable and can be sensitive to hyperparameters. In this paper, we investigate the effect of regularization on the stability and performance of Deep RL algorithms. We show that regularization can significantly improve the stability and robustness of Deep RL algorithms, and it can also lead to improved performance. We conduct experiments on a variety of Deep RL tasks, and we find that our regularized Deep RL algorithms outperform baseline algorithms in terms of both stability and performance.",
    "**Abstract:**\nThe Receptive Field (RF) size has been one of the most important factors for achieving high performance in time series classification. However, choosing an appropriate RF size can be challenging, as it depends on the characteristics of the data. This paper proposes Omni-Scale CNNs, a simple and effective kernel size configuration that automatically adapts to the data. Omni-Scale CNNs use a combination of small and large kernel sizes, capturing both local and global features. This approach is shown to improve accuracy on a variety of time series classification tasks, while being computationally efficient.",
    "**Abstract:**\n\nDistributed optimization is essential for solving large-scale machine learning problems. However, achieving high efficiency and low latency is challenging due to stragglers, which are slow workers that hinder the progress of the distributed system.\n\nThis paper introduces Anytime MiniBatch, a novel approach that exploits stragglers to accelerate distributed optimization in an online setting. By leveraging the gradual completion of stragglers, Anytime MiniBatch maintains a dynamic batch size that adapts to the varying worker availability.\n\nExperimental results demonstrate that Anytime MiniBatch significantly reduces training time compared to existing methods while maintaining high accuracy. This improvement highlights the potential of exploiting stragglers in distributed optimization for efficient and low-latency machine learning applications.",
    "Welcome to WeaSuL 2021, the First Workshop on Weakly Supervised Learning (WeaSuL), co-located with ICLR 2021. \n\nWeaSuL aims to bring together researchers and practitioners working on weakly supervised learning, a rapidly growing field that studies machine learning tasks where the available labels are limited or noisy. The workshop will feature invited talks, contributed papers, and discussions on the latest advances in weakly supervised learning, as well as its applications in various domains such as computer vision, natural language processing, and speech recognition.",
    "**FFPDG: Fast, Fair, and Private Data Generation**\n\nGenerative modeling has gained prominence in synthetic data generation. However, ensuring fairness and privacy in the generated data remains a challenge. FFPDG addresses this by proposing a novel data generation framework that simultaneously achieves these three crucial attributes. FFPDG leverages advanced techniques to generate data that is indistinguishable from real-world data while eliminating biases and safeguarding sensitive information. By combining speed, fairness, and privacy, FFPDG empowers researchers and practitioners with a powerful tool for ethical and responsible synthetic data generation.",
    "Learning from limited samples is prone to errors as the learned model can be sensitive to the training data distribution. In this work, we propose a novel method, Free Lunch for Few-shot Learning: Distribution Calibration, that aims to align the distribution of the support set (few samples) with that of the query set (unseen samples). To this end, we introduce a novel distribution calibration module that utilizes a variational autoencoder to transform the support set's distribution towards the query set's distribution. Extensive experiments on benchmark few-shot learning datasets demonstrate the effectiveness of our method, consistently achieving state-of-the-art performance.",
    "**Title:** On the mapping between Hopfield networks and Restricted Boltzmann Machines\n\n**Abstract:**\nHopfield networks (HNs) and Restricted Boltzmann Machines (RBMs) are two important models at the interface of statistical physics and machine learning. Both models can be seen as energy-based models, which assign an energy to each possible state of the system. This energy is minimized when the system is in a state that is consistent with the training data.\n\nDespite their similarities, HNs and RBMs have different strengths and weaknesses. HNs are simple to train and can store a large number of patterns. However, they can be prone to overfitting and can have difficulty generalizing to new data. RBMs are more complex to train, but they can be more robust to overfitting and can learn more complex relationships.\n\nIn this paper, we present a mapping between HNs and RBMs. This mapping allows us to translate a HN into an equivalent RBM, and vice versa. This mapping provides a new way to understand the relationship between these two models and can be used to improve the performance of both models.",
    "**Persistent Message Passing**\n\n**Abstract:**\n\nGraph neural networks (GNNs) are powerful inductive bias for modelling algorithmic reasoning procedures. However, their efficacy for persistent message passing tasks, which involve passing messages multiple times between nodes in a graph, is not well-studied. This work investigates the design of GNNs for persistent message passing tasks and demonstrates that carefully designed GNNs can achieve superior performance on these tasks. The proposed GNNs exploit the temporal dependencies between messages passed at different time steps to improve accuracy. This work contributes to the development of GNNs for reasoning tasks and provides insights into the design of GNNs for persistent message passing.",
    "**Abstract**\n\nThis paper presents a deep equilibrium model that utilizes implicit layers, where the output of each layer is implicitly defined by an equilibrium point. By introducing implicit layers, the model is able to capture the dynamics of complex systems and learn complex representations in a sequential manner. We prove the global convergence of the proposed model under mild conditions and demonstrate its effectiveness through numerical experiments on various machine learning tasks such as image classification and natural language processing.",
    "The ability to learn continually without forgetting the past tasks is a desired attribute for lifelong learning systems. However, conventional neural networks suffer from catastrophic forgetting when trained on a sequence of tasks. Gradient Projection Memory (GPM) is a recurrent neural network (RNN) architecture with a novel optimization algorithm that tackles this problem. GPM uses a gradient projection layer to constrain the updates of the network parameters, ensuring that the knowledge acquired from previous tasks is preserved while learning new tasks. The optimization algorithm, named Gradient Projection Optimization (GPO), iteratively updates the network parameters by projecting the gradients onto the subspace that preserves the past knowledge. Experiments on a variety of continual learning benchmarks demonstrate that GPM significantly outperforms existing methods in terms of accuracy and forgetting prevention.",
    "In high-dimensional state spaces, the usefulness of Reinforcement Learning (RL) is limited by the problem of reward sparsity. This problem arises when the reward signal is only available at the end of a long sequence of actions, making it difficult for RL algorithms to learn the correct policy. To address this problem, we propose a new reward shaping method called Plan-Based Relaxed Reward Shaping (PBRRS). PBRRS uses a plan-based approach to identify intermediate milestones along the path to the goal, and then provides relaxed rewards for reaching these milestones. This approach allows RL algorithms to learn more quickly and efficiently, even in high-dimensional state spaces. We evaluate PBRRS on a range of goal-directed tasks, and show that it significantly improves the performance of RL algorithms.",
    "**Abstract**\n\nNeural networks are increasingly used to search large, complex spaces for solutions to mathematical problems. However, standard neural network training algorithms can be inefficient and failure-prone when applied to such tasks. This paper presents a new method for improving the exploration capabilities of policy gradient search algorithms, which are commonly used for neural network training. The method is based on the idea of using an auxiliary network to generate candidate solutions that are then evaluated by the policy gradient search algorithm. Experiments show that the proposed method can significantly improve the performance of policy gradient search algorithms on a variety of symbolic optimization problems.",
    "We study training of Convolutional Neural Networks (CNNs) with ReLU activations. We introduce exact convex regularizers of two- and three-layer CNNs. These provide new insights into the training of CNNs: (1) the objective function in both cases is a convex function in a certain space called the local polytope, (2) the number of local minima of the objective function is polynomial in the number of neurons and input features, and (3) thus global optimality can be achieved in polynomial time. Moreover, we show that the convex regularizers can be used to efficiently pretrain CNNs which give state-of-the-art results on MNIST and CIFAR-10.",
    "We consider the problem of finding the best memoryless stochastic policy for an infinite-horizon partially observable Markov decision process (POMDP). We establish that the geometry of the policy space plays a crucial role in determining the complexity of this problem. Specifically, we show that if the policy space is convex, then the problem can be solved efficiently using linear programming. However, if the policy space is non-convex, then the problem is NP-hard. We also provide a geometric interpretation of the value function and show how it can be used to design efficient algorithms for solving the problem.",
    "Stochastic encoders have been used in rate-distortion theory and neural compression because they can be used to represent distributions over symbols in a more compact way than traditional deterministic encoders. In this paper, we discuss the advantages of using stochastic encoders over deterministic encoders in these applications. We show that stochastic encoders can achieve lower distortion for the same rate, and that they can also be used to represent more complex distributions. We also discuss the challenges associated with using stochastic encoders and provide some tips for overcoming them.",
    "Learned transform compression has emerged as a promising approach to image compression. In this paper, we present a novel learned transform compression method that optimizes both the transform matrix and the entropy encoding scheme. Our proposed method, Learned Transform Compression with Optimized Entropy Encoding (LCT-OEE), leverages a variational autoencoder (VAE) to learn the transform matrix, and a reinforcement learning (RL) agent to optimize the entropy encoding scheme. We demonstrate the effectiveness of our method on a variety of image datasets, and show that it achieves state-of-the-art compression performance.",
    "The dynamics of physical systems is often constrained to lower dimensional sub-spaces due to the existence of symmetries. Exploiting symmetries in simulations can lead to significant performance improvements. We propose a novel method, Symmetry Control Neural Networks (SCNNs), to incorporate symmetries into neural network based simulations. SCNNs learn to enforce symmetries by predicting symmetry-preserving transformations of the system state. By conditioning the neural network on these transformations, the network learns to respect the symmetries during simulation. We demonstrate the effectiveness of SCNNs by simulating the dynamics of a rigid body and a fluid flow. SCNNs achieve significantly improved accuracy and stability compared to baseline neural network simulations, while also being more efficient.",
    "In this work, we study the behavior of standard models for community detection when applied to the low-rank projections of the graph Laplacian matrix. We show that these projections can significantly improve the performance of these models, particularly in the presence of noise and outliers. Our findings suggest that low-rank projections can be a valuable tool for enhancing the robustness and accuracy of community detection algorithms.",
    "We propose a new framework of synthesizing data using deep generative models in a differentially private manner. Our approach, PEARL, utilizes a novel combination of private embeddings and adversarial reconstruction learning. By leveraging the privacy-preserving properties of embeddings and the noise-injection mechanism of adversarial training, PEARL effectively mitigates privacy risks while synthesizing realistic data. Extensive experiments on real-world datasets demonstrate that PEARL outperforms existing data synthesis methods in terms of both privacy preservation and data quality.",
    "**Abstract:**\nSelf-supervised visual representation learning aims to learn useful representations without relying on human annotations. Contrastive self-supervised learning is one of the promising approaches in this field. However, contrastive learning methods often suffer from dimensional collapse, i.e., the latent representation collapses to a low-dimensional subspace, which limits the expressive power of the learned representations. In this paper, we analyze the causes of dimensional collapse in contrastive self-supervised learning and propose two novel strategies to mitigate it. Our first strategy, called feature augmentation, introduces additional features to the contrastive loss, which encourages the model to learn more diverse representations. Our second strategy, called representation regularization, adds a penalty term to the contrastive loss that penalizes the model for producing representations that are too similar. We conduct extensive experiments on several benchmark datasets and demonstrate that our proposed strategies effectively mitigate dimensional collapse and improve the performance of contrastive self-supervised learning methods.",
    "We propose a novel self-attention module, Group Equivariant Stand-Alone Self-Attention (GE-SASA), that enforces group equivariance to any arbitrary symmetry group. Unlike previous group equivariant self-attention methods relying on convolutional layers, GE-SASA is purely based on self-attention, making it more flexible and efficient. Our formulation leverages the intrinsic equivariance of self-attention and utilizes a novel group-equivariant projection to preserve symmetry during attention computation. Experiments demonstrate that GE-SASA outperforms existing group equivariant methods on various image classification and segmentation tasks while achieving state-of-the-art results.",
    "We propose the task of disambiguating symbolic expressions in informal STEM documents in the form of mixed text and mathematical expressions. This task is challenging because symbolic expressions in informal documents can be ambiguous, with multiple possible meanings. Our approach leverages contextual information from the surrounding text to resolve these ambiguities. We present a dataset of disambiguated expressions and evaluate our approach using this dataset.",
    "Many machine learning algorithms operate under the assumption that the training data is fairly sampled from the population of interest. However, this assumption is often violated in practice, leading to models that exhibit bias and discrimination against certain groups. Fairness constraints, such as group fairness, have been proposed to address this issue. In this paper, we investigate a new approach for enforcing fairness by interpolating between the predictions of a fair model and an unfair model, both of which are trained on the same data. Our approach, called Fair Mixup, is simple to implement and does not require any additional data or model modifications. We demonstrate that Fair Mixup can significantly improve the fairness of classifiers without sacrificing performance, and we provide theoretical insights into why it works.",
    "Autoregressive models are efficient for image compression, but their output quality can be poor. Our work proposes a novel method for enhancing the quality of autoregressive models by smoothing their probability distributions. This technique, called distribution smoothing, involves applying a Gaussian filter to the predicted probability distribution before decoding. Experiments demonstrate that distribution smoothing significantly improves sample quality, without sacrificing compression efficiency. Our method outperforms existing approaches for image compression, achieving state-of-the-art results on standard benchmarks.",
    "We propose a simple method by which to choose sample weights for problems with highly imbalanced labels. Our method is based on the idea of continuous weight balancing, which adjusts the weights of the samples in each class continuously as the model is trained. This allows the model to focus on the correct classification of the minority class, while still taking into account the information provided by the majority class. We show that our method outperforms existing methods for handling imbalanced data on a variety of benchmark datasets.",
    "In this work, we analyze the reinstatement mechanism introduced by Ritter et al. (2018) to endow a model-based reinforcement learning agent with episodic memory. We show that the mechanism makes the agent develop neurons that encode abstract and episodic representations of the states it visits. We further demonstrate that training with the reinstatement mechanism can improve performance on a variety of sequential decision-making tasks.",
    "Deep Neural Networks (DNNs) are known to be vulnerable to small, adversarially crafted, perturbations. These adversarial examples can cause DNNs to make incorrect predictions, which can have serious consequences in safety-critical applications. In this work, we propose a novel sparse coding frontend for DNNs that improves their robustness to adversarial perturbations. Our frontend consists of a convolutional neural network (CNN) that learns to extract sparse representations of the input data. These sparse representations are then fed into a fully connected neural network (FCN) for classification. We show that our sparse coding frontend can significantly improve the robustness of DNNs to adversarial perturbations, while maintaining their accuracy on clean data.",
    "The rate-distortion-perception function (RDPF) has emerged as a useful tool for analyzing the trade-off between rate, distortion, and perceptual quality in image and video coding. In this paper, we present a coding theorem for the RDPF that provides a fundamental understanding of its properties and limitations. The theorem states that the RDPF of a given source and distortion measure can be represented as the envelope of a family of rate-distortion functions, each corresponding to a different perceptual weight function. This result provides a deeper understanding of the relationship between the RDPF and other fundamental concepts in information theory and perception.",
    "Most graph neural network architectures work by message-passing node vector embeddings over the adjacency matrix, even though recent work has begun to explore simplifying architectures that directly operate on the adjacency matrix itself. Here, we show that even these simplified models fail at a fundamental task: detecting topological features in a graph. We construct a demonstration where GNNs fail to distinguish a highly structured topological object, a near-perfect equilateral triangle, from random noise. We explain that this failure is specific to the geometric nature of the feature and arises when the shape is too concentrated to be captured by vectors of uniform dimensionality. Our result suggests that na\u00efve application of GNNs may lead to inaccurate or even biased results when applied to data with intrinsic geometric structure that is not captured by their representation.",
    "**Abstract:**\n\nPrivacy and security concerns arise in machine learning as it expands into various domains. To address these concerns, this paper proposes a novel training scheme that leverages trusted hardware to preserve privacy and integrity. Trusted hardware, such as a Trusted Platform Module (TPM), provides a secure environment for storing and processing sensitive data. By utilizing this hardware, the proposed scheme ensures that training data remains confidential and protected from unauthorized access. Additionally, it safeguards the integrity of the training process, preventing malicious actors from tampering with the data or models. Experimental results demonstrate the effectiveness of the scheme in preserving privacy and integrity while maintaining high training accuracy.",
    "**Deep Learning Hamiltonian Monte Carlo**\n\nWe generalize the Hamiltonian Monte Carlo algorithm with a stack of neural network layers. The resulting algorithm, Deep Learning Hamiltonian Monte Carlo (DLHMC), can be used to sample from a wide variety of probability distributions, including those that are difficult to sample from using traditional MCMC methods.\n\nDLHMC outperforms traditional HMC on a variety of tasks, including sampling from high-dimensional distributions and distributions with complex geometries. We also show that DLHMC can be used to train deep generative models, such as variational autoencoders.\n\nDLHMC is a powerful new tool for sampling from complex probability distributions. It has the potential to revolutionize the way we perform inference and generate data in a wide range of applications.",
    "Concept bottleneck models map from raw inputs to concepts, and then from concepts to targets. This allows them to learn complex relationships between inputs and outputs, even when the inputs are noisy or incomplete. However, it is not clear whether concept bottleneck models learn as intended. In this paper, we investigate this question by analyzing the behavior of concept bottleneck models on a variety of tasks. We find that concept bottleneck models often learn to exploit spurious correlations in the data, and that this can lead to poor generalization performance. We also find that concept bottleneck models can be sensitive to the choice of hyperparameters, and that this can further degrade generalization performance. Our findings suggest that concept bottleneck models should be used with caution, and that careful attention should be paid to their behavior on held-out data.",
    "In this paper, we propose a novel data poisoning attack against deep reinforcement learning (RL) agents. Our attack, called In-Distribution Trigger (IDT), generates poisoned data points that are indistinguishable from legitimate data but can induce the RL agent to make catastrophic errors. We demonstrate the effectiveness of our attack on various RL tasks, including image classification, object detection, and game playing. We also provide theoretical analysis and experimental results to explain how IDT poisoning can manipulate the decision-making process of RL agents. Our findings highlight the importance of data integrity in RL systems and call for further research on robust defenses against data poisoning attacks.",
    "This paper introduces MONCAE, a neuroevolutionary approach to designing Convolutional Autoencoders (CAEs). MONCAE optimizes both the architecture and hyperparameters of CAEs using a multi-objective evolutionary algorithm, maximizing reconstruction accuracy and minimizing model complexity. Experiments on benchmark datasets demonstrate that MONCAE outperforms existing methods by identifying more compact and accurate CAE architectures.",
    "**Abstract:**\n\nModel-based Reinforcement Learning (MBRL) estimates the true environment through a world model to approximate, and plans actions to maximize long-term performance. In this paper, we propose a novel Probabilistic Model-Based Policy Search (PMBPS) approach that incorporates probabilistic modeling of the world model's uncertainties into the policy search process. PMBPS uses Bayesian inference to update the belief distribution over the world model parameters, and then samples world models from this distribution to generate stochastic policies. By leveraging the uncertainties in the world model, PMBPS is able to learn more robust controllers that are less sensitive to model errors. We demonstrate the effectiveness of PMBPS on a range of simulated and real-world reinforcement learning tasks, where it outperforms existing MBRL methods in terms of both sample efficiency and robustness.",
    "**Training and Generating Neural Networks in Compressed Weight Space**\n\nThis paper proposes a method for training and generating neural networks in compressed weight space. The inputs and/or outputs of some neural networks are weight matrices of other neural networks. This paper introduces a modified backpropagation algorithm and a framework to train feedforward networks in compressed weight space. The paper also proposes a new method of generating feedforward networks.",
    "This paper presents the computational challenge on differential geometry and topology that happened within the International Conference on Learning Representations (ICLR) 2021. The challenge consisted of three tasks: computing the Ricci curvature of a surface, generating random Euclidean minimal surfaces, and learning the topology of a 3D shape. We describe the challenge tasks, the evaluation metrics, and the winning solutions. The challenge has sparked interest in computational geometry and topology within the machine learning community and has helped to foster new collaborations between researchers in both fields.",
    "**Abstract**\n\nTraining time budget and the size of the dataset are key factors affecting the performance of machine learning models. This paper presents an efficient training framework that optimizes model performance under limited resources. By leveraging advanced training techniques such as early stopping, transfer learning, and data augmentation, the framework significantly reduces training time while maintaining high accuracy. Experimental results show that the proposed framework outperforms existing methods in both training efficiency and model performance, making it a valuable tool for resource-constrained applications.",
    "**Abstract:**\n\nThis paper presents SenSeI, a novel framework for fair machine learning by leveraging invariant machine learning. We formulate fair machine learning as a problem of enforcing individual fairness constraints, which are represented as invariants to be maintained during model training. Our approach extends existing invariant optimization techniques to handle complex fairness constraints that encode individual-level protections. By enforcing these invariants, SenSeI ensures the model's adherence to fairness requirements, resulting in predictions that are fair to individuals while maintaining predictive performance.",
    "Despite significant advances, continual learning models still suffer from catastrophic forgetting when exposed to incrementally learned tasks. This occurs when the model's knowledge of previously learned tasks is overwritten by newly learned information. To address this challenge, graph-based continual learning methods have been proposed. These methods represent the model's knowledge as a graph, where nodes represent concepts and edges represent relationships between concepts. By leveraging the graph structure, these methods aim to preserve knowledge of previously learned tasks while incorporating new information. This abstract provides a concise overview of graph-based continual learning, highlighting its potential to mitigate catastrophic forgetting and improve the performance of continual learning models.",
    "**Abstract**\n\nWe provide a rigorous mathematical proof that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and a Laplace kernel are identical. This equivalence implies that both kernels induce the same function space of infinitely smooth functions with analytic derivatives. Our result has important implications for understanding the generalization capabilities of deep neural networks and for developing new learning algorithms that leverage the properties of these kernels.",
    "**Reinforcement Learning with Random Delays**\n\nAction and observation delays commonly occur in many Reinforcement Learning applications, such as remote control, communication networks, and autonomous vehicles. These delays can significantly impact the performance of RL algorithms, as they introduce uncertainty and make it difficult for agents to learn optimal policies. In this work, we propose a novel RL algorithm that can handle random delays effectively. Our algorithm leverages a combination of deep neural networks and dynamic programming to learn a policy that is robust to delays. We evaluate our algorithm on a variety of RL tasks with random delays and demonstrate that it significantly outperforms existing RL algorithms.",
    "Differentially private machine learning (DPML) is a privacy-preserving technique for training machine learning models on sensitive data. We demonstrate that DPML has not yet reached its \"AlexNet moment\" on image data. We find that DPML models trained on CIFAR-10 with state-of-the-art privacy budgets achieve significantly lower accuracy than their non-private counterparts. We investigate the impact of various DPML mechanisms and hyperparameters on accuracy, and find that increasing the privacy budget or using more complex mechanisms does not always lead to improved accuracy. We conclude that DPML needs better features or much more data to achieve competitive accuracy on image data.",
    "We develop an algorithm to train individually fair learning-to-rank (LTR) models. The proposed approach ensures that the ranking produced by the LTR model satisfies individual fairness constraints, which require that the ranking of an individual does not depend on sensitive attributes such as race or gender. Our algorithm is based on a novel optimization framework that combines ranking loss and fairness constraints. We evaluate the proposed algorithm on a real-world dataset and show that it can achieve both high ranking accuracy and individual fairness.",
    "**Abstract**\n\n**Individually Fair Gradient Boosting**\n\nWe consider the task of enforcing individual fairness in gradient boosting. Gradient boosting is a popular machine learning algorithm used for a wide range of tasks, including classification, regression, and ranking. However, gradient boosting can be unfair, as it can make predictions that are biased against certain groups of people.\n\nIn this paper, we propose a new algorithm for gradient boosting that is individually fair. Our algorithm, called FairGB, uses a fairness-aware loss function to train the model. The fairness-aware loss function penalizes the model for making predictions that are unfair to certain groups of people.\n\nWe evaluate FairGB on a variety of datasets, and we show that it is able to achieve high accuracy while also enforcing individual fairness. Our results suggest that FairGB is a promising new algorithm for fair machine learning.",
    "The amount of data, manpower and capital required to understand, evaluate and agree on a single diagnosis is substantial, especially in the case of a new disease during a pandemic. Federated learning creates a network of devices in which the models are learned locally, and only the updates are shared, which avoids sharing individual data. The proposed solution, FedPandemic, is a novel approach towards elementary prognosis of diseases. The use of federated learning reduces data sharing and ensures data privacy, thereby avoiding the drawbacks of traditional approaches. Furthermore, such an approach reduces the cost of labeling data, since only the updates are shared and not the data itself.",
    "Ontologies comprising of concepts, their attributes, and relationships are used in many knowledge-based AI applications. Many methodologies exist to populate ontologies with new instances. However, these methods often use features that do not capture the hierarchical structure of the ontology, leading to suboptimal performance.\n\nTo address this problem, we propose a novel approach that leverages the structural information in the ontology to improve the performance of ontology population.  Our approach employs a relational graph convolutional network (R-GCN) that is aware of the document structure of the input text. The R-GCN uses the hierarchical structure of the ontology to learn representations of concepts and their relationships. These representations are then used to predict the most likely instances for a given input text.\n\nWe evaluate our approach on two real-world datasets and demonstrate that it outperforms existing methods in terms of accuracy and efficiency. Our approach is a promising step towards improving the automation of ontology population and can be used to enhance the performance of a wide range of knowledge-based AI applications.",
    "Imitation Learning algorithms learn a policy from demonstrations of expert behavior. We show that, for a fixed amount of expert demonstrations, there is a tradeoff between the generalization performance of the learned policy and its accuracy on the expert demonstrations. This tradeoff is due to the fact that the imitation learning algorithm must balance the competing goals of following the expert demonstrations and generalizing to unseen situations. We provide theoretical and experimental evidence to support our claim, and we discuss the implications of this tradeoff for the design of imitation learning algorithms.",
    "**Abstract**\n\nBlack-box optimization formulations for biological sequence design have gained attention due to their potential to address complex sequence-function relationships. However, these formulations face challenges in handling likelihood-free scenarios where explicit likelihood functions are unavailable. This paper presents a unifying framework that integrates likelihood-free inference with black-box optimization. The framework utilizes a class of surrogate models known as Gaussian processes to model the objective function and employs Bayesian optimization techniques to guide the search for optimal sequences. The approach is generalizable to various sequence design problems, including protein design, antibody design, and RNA design. Experimental results demonstrate the effectiveness and efficiency of the proposed framework in discovering sequences with improved properties compared to traditional methods.",
    "**Abstract:**\n\nDeep Reinforcement Learning (Deep RL) has gained significant attention due to its promising results in complex control tasks. However, training Deep RL models can be challenging due to instability and overfitting. Regularization techniques play a crucial role in addressing these issues.\n\nThis paper investigates the impact of regularization on policy optimization in Deep RL. We analyze various regularization methods and demonstrate their effectiveness in improving model performance, stability, and generalization. Our findings provide insights into the importance of regularization in Deep RL and offer practical guidelines for its application in different scenarios.",
    "Neural module networks (NMNs) have an architectural bias towards compositionality, making them well-suited for visual question answering (VQA). However, NMNs require gold standard layouts, which can be expensive and time-consuming to obtain. We introduce a novel approach, iterated learning for emergent systematicity (ILES), to train NMNs with noisy layouts. ILES iteratively refines the layout predictions of the NMN and uses the refined layouts to improve the performance of the NMN on the VQA task. We show that ILES achieves state-of-the-art performance on the VQA task, outperforming existing methods by a large margin.",
    "**Abstract**\n\nKnowledge Distillation (KD) is a powerful technique for transferring knowledge from pre-trained teacher models to student models. However, traditional KD methods often result in student models that are easy to distill but difficult to teach students effectively. In this paper, we propose a novel method for creating an \"undistillable\" teacher model that is deliberately designed to be difficult to distill. Our method forces the student model to learn from the teacher model's internal representations, rather than simply memorizing its outputs. We demonstrate the effectiveness of our method on a variety of natural language processing tasks, showing that our undistillable teachers can produce student models that outperform traditional KD methods by a significant margin.",
    "To interpret uncertainty estimates from differentiable probabilistic models, recent work has proposed generating Counterfactual Latent Uncertain Explanations (CLUEs). However, CLUEs can be unstable and difficult to generate, especially for complex models and high-dimensional data. To address these limitations, we propose $\\delta$-CLUEs, a diverse set of explanations that provide a more comprehensive and interpretable view of uncertainty. $\\delta$-CLUEs are generated by perturbing the latent space of the model, and their diversity is controlled by a hyperparameter $\\delta$. We show that $\\delta$-CLUEs are more stable and easier to generate than CLUEs, and they provide a richer understanding of uncertainty in a variety of tasks, including image classification and conditional image generation."
]